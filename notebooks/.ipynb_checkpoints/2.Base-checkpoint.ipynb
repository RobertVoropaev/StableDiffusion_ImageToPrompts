{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "981d5a96",
   "metadata": {},
   "source": [
    "# 2.Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3313abeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image\n",
    "from matplotlib import pyplot as plt \n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import json\n",
    "\n",
    "from pathlib import Path\n",
    "from scipy.spatial import distance\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append('../input/sentence-transformers-222/sentence-transformers')\n",
    "from sentence_transformers import SentenceTransformer, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97ed597f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../input/DiffusionDB_part-000001/part-000001.json\") as f:\n",
    "    prompts = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7721c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "st_model = SentenceTransformer('/home/rv/data/sentence-transformers-222/all-MiniLM-L6-v2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a00cdaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "meta_categories = ResNet50_Weights.IMAGENET1K_V1.meta[\"categories\"]\n",
    "\n",
    "model = resnet50()\n",
    "model.load_state_dict(torch.load(\"/home/rv/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef07335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batches(l, batch_size=16):\n",
    "    for i in range(0, len(l), batch_size):\n",
    "        yield l[i:i + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a7191254",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "images_path = Path(\"../input/DiffusionDB_part-000001/\")\n",
    "\n",
    "general_words = [\"fine details\", \"hyperrealism\", \"highly detailed\", \"hyperdetailed\", \"unreal engine 5\", \"ray tracing\"]\n",
    "\n",
    "k = 7\n",
    "\n",
    "emb_arr = []\n",
    "for image_path_arr in make_batches(os.listdir(images_path), batch_size=32):\n",
    "    img_arr = []\n",
    "    for img_path in image_path_arr:\n",
    "        if \".png\" not in img_path:\n",
    "            continue\n",
    "            \n",
    "        img = Image.open(str(images_path / img_path))\n",
    "\n",
    "        img = preprocess(img)\n",
    "        img_arr.append(img)\n",
    "\n",
    "    batch = torch.tensor([tens.numpy() for tens in img_arr])\n",
    "\n",
    "    prediction = model(batch).softmax(1)\n",
    "    categories = []\n",
    "    for classes in prediction.topk(k, dim=1).indices.numpy():\n",
    "        for class_id in classes:\n",
    "            category_name = meta_categories[class_id]\n",
    "            categories.append(category_name)\n",
    "\n",
    "        category_embeddings = st_model.encode(\", \".join(categories + general_words)).flatten()\n",
    "        emb_arr.append(category_embeddings)\n",
    "        \n",
    "emb_arr = np.array(emb_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a85984c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_emb_arr = []\n",
    "for image_path_arr in make_batches(os.listdir(images_path), batch_size=32):\n",
    "    for img_path in image_path_arr:\n",
    "        if \".png\" not in img_path:\n",
    "            continue\n",
    "            \n",
    "        prompt_embeddings = st_model.encode(prompts[img_path][\"p\"]).flatten()\n",
    "        true_emb_arr.append(prompt_embeddings)\n",
    "        \n",
    "true_emb_arr = np.array(true_emb_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "37a534c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert true_emb_arr.shape == emb_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "af38d7db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2352892928867368"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_res = 0\n",
    "for i in range(len(true_emb_arr)):\n",
    "    sim_res += 1 - distance.cosine(true_emb_arr[i], emb_arr[i])\n",
    "sim_res / (i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f86fd92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
