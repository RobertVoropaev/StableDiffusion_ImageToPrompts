{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91b7e42a",
   "metadata": {},
   "source": [
    "# 7.Img2Emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2cf9252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.0.0) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "2023-05-01 13:16:17.883907: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-01 13:16:18.013009: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-01 13:16:18.480192: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import json\n",
    "import glob\n",
    "import gc\n",
    "import random\n",
    "import time\n",
    "import unicodedata\n",
    "import traceback\n",
    "import datetime\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt \n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from scipy.spatial import distance\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models import (\n",
    "    vit_b_16, ViT_B_16_Weights, \n",
    "    vit_l_16, ViT_L_16_Weights,\n",
    "    vit_h_14, ViT_H_14_Weights,\n",
    "    regnet_y_32gf, RegNet_Y_32GF_Weights,\n",
    "    regnet_y_128gf, RegNet_Y_128GF_Weights,\n",
    "    regnet_y_16gf, RegNet_Y_16GF_Weights,\n",
    "    efficientnet_v2_l, EfficientNet_V2_L_Weights,\n",
    "    efficientnet_v2_m, EfficientNet_V2_M_Weights,\n",
    "    convnext_large, ConvNeXt_Large_Weights,\n",
    "    swin_v2_b, Swin_V2_B_Weights\n",
    ")\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import (\n",
    "    StepLR, MultiStepLR, \n",
    "    ConstantLR, LinearLR, \n",
    "    ExponentialLR, PolynomialLR, \n",
    "    CosineAnnealingLR, CosineAnnealingWarmRestarts, \n",
    "    CyclicLR, OneCycleLR, \n",
    "    ReduceLROnPlateau\n",
    ")\n",
    "\n",
    "sys.path.append('../input/sentence-transformers-222/sentence-transformers')\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca0f7af",
   "metadata": {},
   "source": [
    "## CFG "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9119912",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_config = {\n",
    "    \"vit_b_16\": {\n",
    "        True: 256,\n",
    "        False: 16\n",
    "    },\n",
    "    \"vit_b_16_linear\": {\n",
    "        True: 256,\n",
    "        False: 48\n",
    "    },\n",
    "    \"vit_l_16\": {\n",
    "        True: 16,\n",
    "        False: 1\n",
    "    }, \n",
    "    \"--vit_h_14\": {\n",
    "        True: 4,\n",
    "        False: 1\n",
    "    },\n",
    "    \"regnet_y_16gf\": {\n",
    "        True: 64,\n",
    "        False: 26\n",
    "    },\n",
    "    \"regnet_y_16gf_linear\": {\n",
    "        True: 64,\n",
    "        False: 26\n",
    "    },\n",
    "    \"regnet_y_32gf\": {\n",
    "        True: 16,\n",
    "        False: 6\n",
    "    },\n",
    "    \"regnet_y_32gf_linear\": {\n",
    "        True: 16,\n",
    "        False: 20\n",
    "    },\n",
    "    \"--regnet_y_128gf\": {\n",
    "        True: 4,\n",
    "        False: 1\n",
    "    },\n",
    "    \"--regnet_y_128gf_linear\": {\n",
    "        True: 4,\n",
    "        False: 1\n",
    "    },\n",
    "    \"--efficientnet_v2_l\": {\n",
    "        True: 64,\n",
    "        False: 3\n",
    "    },\n",
    "    \"--efficientnet_v2_m\": {\n",
    "        True: 64,\n",
    "        False: 6\n",
    "    },\n",
    "    \"--convnext_large\": {\n",
    "        True: 64,\n",
    "        False: 12\n",
    "    },\n",
    "    \"--swin_v2_b\": {\n",
    "        True: 256,\n",
    "        False: 16\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0355358b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'img_256_1280_ratio_2_prompt_5_100_dupl_2m_20_sd2v1_False_sd2v2_30_sd3_30_model_regnet_y_32gf_linear_lr_1e_05_sch_CyclicLR'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CFG:\n",
    "    seed = 42\n",
    "    text_emb_size = 384\n",
    "    is_kaggle = (os.environ.get('PWD') == '/kaggle/working')\n",
    "    \n",
    "    train_files_dir = \"img2emb-data\"\n",
    "    save_model = True \n",
    "    \n",
    "    # DATASET FILTERS\n",
    "    img_size_min = 256\n",
    "    img_size_max = 1280\n",
    "    img_max_ratio_diff = 2\n",
    "    prompt_words_min = 5\n",
    "    prompt_words_max = 100\n",
    "    prompt_is_english = True\n",
    "    \n",
    "    drop_duplicates_by_head = True\n",
    "    drop_duplicates_by_tail = False\n",
    "    \n",
    "    drop_duplicates_char_len_2m = 20\n",
    "    drop_duplicates_char_len_sd2v1 = 30\n",
    "    drop_duplicates_char_len_sd2v2 = 30\n",
    "    drop_duplicates_char_len_sd3 = 30\n",
    "\n",
    "    add_sd2_v1 = False\n",
    "    add_sd2_v2 = True\n",
    "    add_sd3 = True\n",
    "    \n",
    "    drop_duplicates_char_len_sd2v1_name = drop_duplicates_char_len_sd2v1 if add_sd2_v1 else False\n",
    "    drop_duplicates_char_len_sd2v2_name = drop_duplicates_char_len_sd2v2 if add_sd2_v2 else False\n",
    "    drop_duplicates_char_len_sd3_name = drop_duplicates_char_len_sd3 if add_sd3 else False\n",
    "    \n",
    "    \n",
    "    img_dataset_name = f\"img_{img_size_min}_{img_size_max}_ratio_{img_max_ratio_diff}\".replace(\".\", \"_\")\n",
    "    prompt_dataset_name = f\"prompt_{prompt_words_min}_{prompt_words_max}\"\n",
    "    dupl_dataset_name = f\"dupl_2m_{drop_duplicates_char_len_2m}_sd2v1_{drop_duplicates_char_len_sd2v1_name}_sd2v2_{drop_duplicates_char_len_sd2v2_name}_sd3_{drop_duplicates_char_len_sd3_name}\"\n",
    "    dataset_name = f\"{img_dataset_name}_{prompt_dataset_name}_{dupl_dataset_name}\"\n",
    "    \n",
    "    # TRAIN TEST SPLIT\n",
    "    img_model_test_size = 0.05\n",
    "    \n",
    "    # MODEL\n",
    "    # \"vit_b_16\", \"vit_b_16_linear\", \"regnet_y_16gf\", \"regnet_y_32gf\", \"regnet_y_32gf_linear\", \"regnet_y_16gf_linear\"\n",
    "    img_model_name = \"regnet_y_32gf_linear\" \n",
    "    loss_name = \"cosine\"\n",
    "    lr = 1e-5\n",
    "    # \"None\", \"StepLR\", \"ExponentialLR\", \"CosineAnnealingLR\", \"CosineAnnealingWarmRestartsLR\", \"CyclicLR\" \n",
    "    lr_scheduler_name = \"CyclicLR\" \n",
    "    train_only_head = False\n",
    "    \n",
    "    # AUGMENTATIONS\n",
    "    train_aug = True\n",
    "    test_flip = True\n",
    "    aug_name = f\"flip_{int(train_aug)}\"\n",
    "    \n",
    "    model_name = f\"model_{img_model_name}_lr_{lr:.0e}_sch_{lr_scheduler_name}\".replace(\"-\", \"_\")\n",
    "    \n",
    "    # RESOURCES\n",
    "    batch_size = batch_size_config[img_model_name][is_kaggle]\n",
    "    num_workers = batch_size if not is_kaggle else 2\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # TRAIN CONFIG\n",
    "    full_train_epoch_num = 100\n",
    "    max_epoch_num = full_train_epoch_num * 10\n",
    "    full_val_epoch_num = 5\n",
    "    early_stopping_patience = full_val_epoch_num * 5\n",
    "    \n",
    "    # PATHS\n",
    "    \n",
    "    train_name = f\"{dataset_name}_{model_name}\"\n",
    "\n",
    "set_seed(CFG.seed)\n",
    "CFG.train_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d4824e",
   "metadata": {},
   "source": [
    "## Val functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6f000e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_model(img_model_name):\n",
    "    if img_model_name == \"regnet_y_16gf\":\n",
    "        if CFG.is_kaggle:\n",
    "            model = regnet_y_16gf()\n",
    "        else:\n",
    "            weights = RegNet_Y_16GF_Weights.IMAGENET1K_SWAG_E2E_V1\n",
    "            model = regnet_y_16gf(weights=weights)\n",
    "        model.fc = torch.nn.Linear(3024, CFG.text_emb_size)\n",
    "        \n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.Resize(224, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    if img_model_name == \"regnet_y_16gf_linear\":\n",
    "        if CFG.is_kaggle:\n",
    "            model = regnet_y_16gf()\n",
    "        else:\n",
    "            weights = RegNet_Y_16GF_Weights.IMAGENET1K_SWAG_LINEAR_V1\n",
    "            model = regnet_y_16gf(weights=weights)\n",
    "        model.fc = torch.nn.Linear(3024, CFG.text_emb_size)\n",
    "        \n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.Resize(224, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    elif img_model_name == \"regnet_y_32gf\":\n",
    "        if CFG.is_kaggle:\n",
    "            model = regnet_y_32gf()\n",
    "        else:\n",
    "            weights = RegNet_Y_32GF_Weights.IMAGENET1K_SWAG_E2E_V1\n",
    "            model = regnet_y_32gf(weights=weights)\n",
    "        model.fc = torch.nn.Linear(3712, CFG.text_emb_size)\n",
    "        \n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.Resize(384, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "            transforms.CenterCrop(384),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        \n",
    "    elif img_model_name == \"regnet_y_32gf_linear\":\n",
    "        if CFG.is_kaggle:\n",
    "            model = regnet_y_32gf()\n",
    "        else:\n",
    "            weights = RegNet_Y_32GF_Weights.IMAGENET1K_SWAG_LINEAR_V1\n",
    "            model = regnet_y_32gf(weights=weights)\n",
    "        model.fc = torch.nn.Linear(3712, CFG.text_emb_size)\n",
    "        \n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.Resize(224, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    \n",
    "    elif img_model_name == \"regnet_y_128gf\":\n",
    "        if CFG.is_kaggle:\n",
    "            model = regnet_y_128gf()\n",
    "        else:\n",
    "            weights = RegNet_Y_128GF_Weights.IMAGENET1K_SWAG_E2E_V1\n",
    "            model = regnet_y_128gf(weights=weights)\n",
    "        model.fc = torch.nn.Linear(7392, CFG.text_emb_size)\n",
    "        \n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.Resize(384, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "            transforms.CenterCrop(384),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    elif img_model_name == \"regnet_y_128gf_linear\":\n",
    "        if CFG.is_kaggle:\n",
    "            model = regnet_y_128gf()\n",
    "        else:\n",
    "            weights = RegNet_Y_128GF_Weights.IMAGENET1K_SWAG_LINEAR_V1\n",
    "            model = regnet_y_128gf(weights=weights)\n",
    "        model.fc = torch.nn.Linear(7392, CFG.text_emb_size)\n",
    "        \n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.Resize(224, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        \n",
    "    elif img_model_name == \"vit_b_16\":\n",
    "        if CFG.is_kaggle:\n",
    "            model = vit_b_16(image_size=384)\n",
    "        else:\n",
    "            weights = ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1\n",
    "            model = vit_b_16(weights=weights)\n",
    "        model.heads.head = torch.nn.Linear(768, CFG.text_emb_size)\n",
    "        \n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.Resize(384, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "            transforms.CenterCrop(384),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    \n",
    "    elif img_model_name == \"vit_b_16_linear\":\n",
    "        if CFG.is_kaggle:\n",
    "            model = vit_b_16(image_size=224)\n",
    "        else:\n",
    "            weights = ViT_B_16_Weights.IMAGENET1K_SWAG_LINEAR_V1\n",
    "            model = vit_b_16(weights=weights)\n",
    "        model.heads.head = torch.nn.Linear(768, CFG.text_emb_size)\n",
    "        \n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.Resize(224, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    \n",
    "    elif img_model_name == \"vit_l_16\":\n",
    "        if CFG.is_kaggle:\n",
    "            model = vit_l_16(image_size=512)\n",
    "        else:\n",
    "            weights = ViT_L_16_Weights.IMAGENET1K_SWAG_E2E_V1\n",
    "            model = vit_l_16(weights=weights)\n",
    "\n",
    "        model.heads.head = torch.nn.Linear(1024, CFG.text_emb_size)\n",
    "    \n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.Resize(512, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "            transforms.CenterCrop(512),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    elif img_model_name == \"vit_h_14\":\n",
    "        if CFG.is_kaggle:\n",
    "            model = vit_h_14(image_size=512)\n",
    "        else:\n",
    "            weights = ViT_H_14_Weights.IMAGENET1K_SWAG_E2E_V1\n",
    "            model = vit_h_14(weights=weights)\n",
    "\n",
    "        model.heads.head = torch.nn.Linear(1280, CFG.text_emb_size)\n",
    "    \n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.Resize(518, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "            transforms.CenterCrop(518),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    elif img_model_name == \"efficientnet_v2_l\":\n",
    "        if CFG.is_kaggle:\n",
    "            model = efficientnet_v2_l(image_size=480)\n",
    "        else:\n",
    "            weights = EfficientNet_V2_L_Weights.IMAGENET1K_V1\n",
    "            model = efficientnet_v2_l(weights=weights)\n",
    "\n",
    "        model.classifier[1] = torch.nn.Linear(1280, CFG.text_emb_size)\n",
    "    \n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.Resize(480, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "            transforms.CenterCrop(480),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], \n",
    "                                 std=[0.5, 0.5, 0.5]),\n",
    "        ])\n",
    "\n",
    "    elif img_model_name == \"efficientnet_v2_m\":\n",
    "        if CFG.is_kaggle:\n",
    "            model = efficientnet_v2_m(image_size=480)\n",
    "        else:\n",
    "            weights = EfficientNet_V2_M_Weights.IMAGENET1K_V1\n",
    "            model = efficientnet_v2_m(weights=weights)\n",
    "\n",
    "        model.classifier[1] = torch.nn.Linear(1280, CFG.text_emb_size)\n",
    "    \n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.Resize(480, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.CenterCrop(480),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    elif img_model_name == \"convnext_large\":\n",
    "        if CFG.is_kaggle:\n",
    "            model = convnext_large(image_size=224)\n",
    "        else:\n",
    "            weights = ConvNeXt_Large_Weights.IMAGENET1K_V1\n",
    "            model = convnext_large(weights=weights)\n",
    "\n",
    "        model.classifier[2] = torch.nn.Linear(1536, CFG.text_emb_size)\n",
    "    \n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.Resize(224, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    elif img_model_name == \"swin_v2_b\":\n",
    "        if CFG.is_kaggle:\n",
    "            model = swin_v2_b(image_size=256)\n",
    "        else:\n",
    "            weights = Swin_V2_B_Weights.IMAGENET1K_V1\n",
    "            model = swin_v2_b(weights=weights)\n",
    "\n",
    "        model.head = torch.nn.Linear(1024, CFG.text_emb_size)\n",
    "    \n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.Resize(256, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "            transforms.CenterCrop(256),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    \n",
    "    model.to(CFG.device)\n",
    "    return model, preprocess\n",
    "\n",
    "def create_submission(pred_arr, img_names):\n",
    "    imgIds = [i.split('.')[0] for i in img_names]\n",
    "\n",
    "    EMBEDDING_LENGTH = CFG.text_emb_size\n",
    "    eIds = list(range(EMBEDDING_LENGTH))\n",
    "\n",
    "    imgId_eId = [\n",
    "        '_'.join(map(str, i)) for i in zip(\n",
    "            np.repeat(imgIds, EMBEDDING_LENGTH),\n",
    "            np.tile(range(EMBEDDING_LENGTH), len(imgIds)))]\n",
    "    \n",
    "    submission = pd.DataFrame(\n",
    "                    index=imgId_eId,\n",
    "                    data=np.array(pred_arr).flatten(),\n",
    "                    columns=['val']).rename_axis('imgId_eId')\n",
    "    return submission\n",
    "\n",
    "class CustomDataSet(Dataset):\n",
    "    def __init__(self, data_dir, img2prompt, img_preprocess):\n",
    "        self.data_dir = data_dir\n",
    "        self.img_names = list(img2prompt.keys())\n",
    "        self.img2prompt = img2prompt\n",
    "        self.img_preprocess = img_preprocess\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_names[idx]\n",
    "        img_path = os.path.join(self.data_dir, img_name)\n",
    "        img = Image.open(img_path)\n",
    "        img_emb = self.img_preprocess(img)\n",
    "        \n",
    "        prompt = str(self.img2prompt[img_name])\n",
    "        \n",
    "        return img_name, img_emb, prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295b8ae7",
   "metadata": {},
   "source": [
    "## Train functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0349eaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_config(train_size, val_size):\n",
    "    max_batches_per_epoch_train = train_size // CFG.batch_size // CFG.full_train_epoch_num\n",
    "    max_batches_per_epoch_val = val_size // CFG.batch_size // CFG.full_val_epoch_num\n",
    "    return max_batches_per_epoch_train, max_batches_per_epoch_val\n",
    "\n",
    "def filter_metadata(df, \n",
    "                    img_size_min, img_size_max, \n",
    "                    img_max_ratio_diff, \n",
    "                    prompt_words_min, prompt_words_max, \n",
    "                    prompt_is_english,\n",
    "                    drop_duplicates_by_head, \n",
    "                    drop_duplicates_by_tail, \n",
    "                    drop_duplicates_char_len):\n",
    "    def is_english_only(string):\n",
    "        for s in string:\n",
    "            cat = unicodedata.category(s)         \n",
    "            if (cat not in ['Ll', 'Lu', 'Nd', 'Po', 'Pd', 'Zs']) or (not cat.isascii()):\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    df = df.copy()\n",
    "    df[\"prompt\"] = df[\"prompt\"].astype(str)\n",
    "    \n",
    "    df[\"size_ratio\"] = df[\"height\"] / df[\"width\"]\n",
    "    df['prompt'] = df['prompt'].str.strip()\n",
    "    df[\"num_words\"] = df['prompt'].str.split(\" \").apply(len)\n",
    "    df[\"is_english\"] = df[\"prompt\"].apply(is_english_only)\n",
    "    \n",
    "    img_hw_cond = (\n",
    "        df[\"width\"].between(img_size_min, img_size_max) & \n",
    "        df[\"height\"].between(img_size_min, img_size_max)\n",
    "    )\n",
    "    img_ratio_cond = df[\"size_ratio\"].between(1/img_max_ratio_diff, img_max_ratio_diff)\n",
    "    prompt_empty_cond = (df[\"prompt\"] != \"\")\n",
    "    prompt_num_words_cond = df[\"num_words\"].between(prompt_words_min, prompt_words_max)\n",
    "\n",
    "    if prompt_is_english:\n",
    "        df = df[df[\"is_english\"]]\n",
    "    \n",
    "    if drop_duplicates_by_head:\n",
    "        df['head'] = df['prompt'].str[:drop_duplicates_char_len]\n",
    "        df.drop_duplicates(subset='head', inplace=True)\n",
    "    \n",
    "    if drop_duplicates_by_tail:\n",
    "        df['tail'] = df['prompt'].str[-drop_duplicates_char_len:]\n",
    "        df.drop_duplicates(subset='tail', inplace=True)\n",
    "    \n",
    "    \n",
    "    df = df[\n",
    "        img_hw_cond &\n",
    "        img_ratio_cond &\n",
    "        prompt_empty_cond &\n",
    "        prompt_num_words_cond\n",
    "    ][[\"image_name\", \"prompt\"]]\n",
    "    \n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def get_loss(loss_name):\n",
    "    if loss_name == \"cosine\":\n",
    "        loss_fn = torch.nn.CosineEmbeddingLoss()\n",
    "        return lambda pred, true: loss_fn(pred, true, torch.ones(pred.size(0)).to(CFG.device))\n",
    "    \n",
    "def get_scheduler(scheduler_name):\n",
    "    overfiting_epoch = int(CFG.full_train_epoch_num)\n",
    "    lr_start = CFG.lr\n",
    "    \n",
    "    if scheduler_name == \"None\":\n",
    "        scheduler = None\n",
    "    elif scheduler_name == \"StepLR\":\n",
    "        scheduler = lambda optimizer: StepLR(\n",
    "            optimizer, step_size = overfiting_epoch * 0.5, gamma = 0.5\n",
    "        )\n",
    "    elif scheduler_name == \"ExponentialLR\":\n",
    "        scheduler = lambda optimizer: ExponentialLR(\n",
    "            optimizer, gamma = 1.05\n",
    "        )\n",
    "    elif scheduler_name == \"CosineAnnealingLR\":\n",
    "        scheduler = lambda optimizer: CosineAnnealingLR(\n",
    "            optimizer, T_max=overfiting_epoch, eta_min=lr_start * 0.1\n",
    "        )\n",
    "    elif scheduler_name == \"CosineAnnealingWarmRestartsLR\":\n",
    "        scheduler = lambda optimizer: CosineAnnealingWarmRestarts(\n",
    "            optimizer, T_0=overfiting_epoch, T_mult=1, eta_min=lr_start * 0.01\n",
    "        )\n",
    "    elif scheduler_name == \"CyclicLR\":\n",
    "        scheduler = lambda optimizer: CyclicLR(\n",
    "            optimizer, \n",
    "            base_lr =lr_start / 10, max_lr = lr_start * 10, \n",
    "            step_size_up = overfiting_epoch * 0.5, \n",
    "            mode = \"triangular2\", \n",
    "            cycle_momentum=False\n",
    "        )\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eac428",
   "metadata": {},
   "source": [
    "## Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48781013",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_236048/1979728441.py:50: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df = df[\n",
      "/tmp/ipykernel_236048/1979728441.py:50: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df = df[\n",
      "/tmp/ipykernel_236048/1979728441.py:50: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  df = df[\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DiffusionDB_2M/2217ccbd-a1c6-47ac-9a2d-7964972...</td>\n",
       "      <td>a portrait of a female robot made from code, v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DiffusionDB_2M/dc71658a-5e4b-4dca-861a-e153551...</td>\n",
       "      <td>only memories remain, trending on artstation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DiffusionDB_2M/48eb7e17-a3cf-4eb8-96a9-d8e3e23...</td>\n",
       "      <td>dream swimming pool with nobody</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DiffusionDB_2M/601d9792-eccd-4850-97a7-edbe91d...</td>\n",
       "      <td>a dog doing weights. epic oil painting.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DiffusionDB_2M/3c586acb-14dc-43df-8900-954c336...</td>\n",
       "      <td>a dog doing weights on fire. epic oil painting.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689334</th>\n",
       "      <td>sd3/artifacts/sd-img-to-prompts:v29/00995.png</td>\n",
       "      <td>portrait of modern darna, sonequa martin - gre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689335</th>\n",
       "      <td>sd3/artifacts/sd-img-to-prompts:v29/00996.png</td>\n",
       "      <td>a greenhouse with deep green and purple glowin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689336</th>\n",
       "      <td>sd3/artifacts/sd-img-to-prompts:v29/00998.png</td>\n",
       "      <td>1 9 2 0 s color spirit photography 0 9 1 1 2 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689337</th>\n",
       "      <td>sd3/artifacts/sd-img-to-prompts:v29/00999.png</td>\n",
       "      <td>gary busey doing a sweet skateboard trick off ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689338</th>\n",
       "      <td>sd3/artifacts/sd-img-to-prompts:v29/01000.png</td>\n",
       "      <td>a portrait of a hero in a disney movie, oil pa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>689339 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image_name  \\\n",
       "0       DiffusionDB_2M/2217ccbd-a1c6-47ac-9a2d-7964972...   \n",
       "1       DiffusionDB_2M/dc71658a-5e4b-4dca-861a-e153551...   \n",
       "2       DiffusionDB_2M/48eb7e17-a3cf-4eb8-96a9-d8e3e23...   \n",
       "3       DiffusionDB_2M/601d9792-eccd-4850-97a7-edbe91d...   \n",
       "4       DiffusionDB_2M/3c586acb-14dc-43df-8900-954c336...   \n",
       "...                                                   ...   \n",
       "689334      sd3/artifacts/sd-img-to-prompts:v29/00995.png   \n",
       "689335      sd3/artifacts/sd-img-to-prompts:v29/00996.png   \n",
       "689336      sd3/artifacts/sd-img-to-prompts:v29/00998.png   \n",
       "689337      sd3/artifacts/sd-img-to-prompts:v29/00999.png   \n",
       "689338      sd3/artifacts/sd-img-to-prompts:v29/01000.png   \n",
       "\n",
       "                                                   prompt  \n",
       "0       a portrait of a female robot made from code, v...  \n",
       "1            only memories remain, trending on artstation  \n",
       "2                         dream swimming pool with nobody  \n",
       "3                 a dog doing weights. epic oil painting.  \n",
       "4         a dog doing weights on fire. epic oil painting.  \n",
       "...                                                   ...  \n",
       "689334  portrait of modern darna, sonequa martin - gre...  \n",
       "689335  a greenhouse with deep green and purple glowin...  \n",
       "689336  1 9 2 0 s color spirit photography 0 9 1 1 2 1...  \n",
       "689337  gary busey doing a sweet skateboard trick off ...  \n",
       "689338  a portrait of a hero in a disney movie, oil pa...  \n",
       "\n",
       "[689339 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_dir = Path(\"../input/\")\n",
    "metadata_2m = pd.read_parquet(train_data_dir / \"DiffusionDB_2M/metadata.parquet\")\n",
    "metadata_2m[\"image_name\"] = \"DiffusionDB_2M/\" + metadata_2m[\"image_name\"]\n",
    "metadata_2m = filter_metadata(\n",
    "    metadata_2m, \n",
    "    img_size_min=CFG.img_size_min, \n",
    "    img_size_max=CFG.img_size_max, \n",
    "    img_max_ratio_diff=CFG.img_max_ratio_diff, \n",
    "    prompt_words_min=CFG.prompt_words_min, \n",
    "    prompt_words_max=CFG.prompt_words_max,\n",
    "    prompt_is_english=CFG.prompt_is_english,\n",
    "    drop_duplicates_by_head=CFG.drop_duplicates_by_head,\n",
    "    drop_duplicates_by_tail=CFG.drop_duplicates_by_tail,\n",
    "    drop_duplicates_char_len=CFG.drop_duplicates_char_len_2m\n",
    ")\n",
    "\n",
    "metadata = metadata_2m\n",
    "\n",
    "if CFG.add_sd2_v1:\n",
    "    metadata_sd2_v1 = pd.read_parquet(train_data_dir / \"gustavosta-sd2-v1/metadata.parquet\")\n",
    "    metadata_sd2_v1[\"image_name\"] = \"gustavosta-sd2-v1/\" + metadata_sd2_v1[\"image_name\"]\n",
    "    metadata_sd2_v1[\"height\"] = 512\n",
    "    metadata_sd2_v1[\"width\"] = 512\n",
    "    metadata_sd2_v1 = filter_metadata(\n",
    "        metadata_sd2_v1, \n",
    "        img_size_min=CFG.img_size_min, \n",
    "        img_size_max=CFG.img_size_max, \n",
    "        img_max_ratio_diff=CFG.img_max_ratio_diff, \n",
    "        prompt_words_min=CFG.prompt_words_min, \n",
    "        prompt_words_max=CFG.prompt_words_max,\n",
    "        prompt_is_english=CFG.prompt_is_english,\n",
    "        drop_duplicates_by_head=CFG.drop_duplicates_by_head,\n",
    "        drop_duplicates_by_tail=CFG.drop_duplicates_by_tail,\n",
    "        drop_duplicates_char_len=CFG.drop_duplicates_char_len_sd2v1\n",
    "    )\n",
    "    \n",
    "    metadata = pd.concat([metadata, metadata_sd2_v1], ignore_index=True)\n",
    "\n",
    "if CFG.add_sd2_v2:\n",
    "    metadata_sd2_v2 = pd.read_parquet(train_data_dir / \"gustavosta-sd2-v2/metadata.parquet\")\n",
    "    metadata_sd2_v2[\"image_name\"] = \"gustavosta-sd2-v2/\" + metadata_sd2_v2[\"image_name\"]\n",
    "    metadata_sd2_v2[\"height\"] = 512\n",
    "    metadata_sd2_v2[\"width\"] = 512\n",
    "    metadata_sd2_v2 = filter_metadata(\n",
    "        metadata_sd2_v2, \n",
    "        img_size_min=CFG.img_size_min, \n",
    "        img_size_max=CFG.img_size_max, \n",
    "        img_max_ratio_diff=CFG.img_max_ratio_diff, \n",
    "        prompt_words_min=CFG.prompt_words_min, \n",
    "        prompt_words_max=CFG.prompt_words_max,\n",
    "        prompt_is_english=CFG.prompt_is_english,\n",
    "        drop_duplicates_by_head=CFG.drop_duplicates_by_head,\n",
    "        drop_duplicates_by_tail=CFG.drop_duplicates_by_tail,\n",
    "        drop_duplicates_char_len=CFG.drop_duplicates_char_len_sd2v2\n",
    "    )\n",
    "    \n",
    "    metadata = pd.concat([metadata, metadata_sd2_v2], ignore_index=True)\n",
    "    \n",
    "if CFG.add_sd3:\n",
    "    metadata_sd3 = pd.read_csv(train_data_dir / \"sd3/metadata.csv\")\n",
    "    metadata_sd3[\"image_name\"] = \"sd3/\" + metadata_sd3[\"image_path\"]\n",
    "    del metadata_sd3[\"image_path\"]\n",
    "    metadata_sd3[\"height\"] = 512\n",
    "    metadata_sd3[\"width\"] = 512\n",
    "    metadata_sd3 = filter_metadata(\n",
    "        metadata_sd3, \n",
    "        img_size_min=CFG.img_size_min, \n",
    "        img_size_max=CFG.img_size_max, \n",
    "        img_max_ratio_diff=CFG.img_max_ratio_diff, \n",
    "        prompt_words_min=CFG.prompt_words_min, \n",
    "        prompt_words_max=CFG.prompt_words_max,\n",
    "        prompt_is_english=CFG.prompt_is_english,\n",
    "        drop_duplicates_by_head=CFG.drop_duplicates_by_head,\n",
    "        drop_duplicates_by_tail=CFG.drop_duplicates_by_tail,\n",
    "        drop_duplicates_char_len=CFG.drop_duplicates_char_len_sd3\n",
    "    )\n",
    "    \n",
    "    metadata = pd.concat([metadata, metadata_sd3], ignore_index=True)\n",
    "    \n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ee87e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DiffusionDB_2M       625544\n",
       "gustavosta-sd2-v2     40371\n",
       "sd3                   23424\n",
       "Name: image_name, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata[\"image_name\"].str.split(\"/\").str[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67f30e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "654872 34467\n",
      "327 344\n",
      "6540 6880\n"
     ]
    }
   ],
   "source": [
    "full_prompt = metadata[[\"image_name\", \"prompt\"]].values\n",
    "train_prompt, val_prompt = train_test_split(\n",
    "    full_prompt, \n",
    "    test_size=CFG.img_model_test_size, \n",
    "    random_state=CFG.seed,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "CFG.dataset_train_size = len(train_prompt)\n",
    "CFG.dataset_val_size = len(val_prompt)\n",
    "CFG.max_batches_per_epoch_train, CFG.max_batches_per_epoch_val = get_train_config(CFG.dataset_train_size, \n",
    "                                                                                  CFG.dataset_val_size)\n",
    "\n",
    "print(CFG.dataset_train_size, CFG.dataset_val_size)\n",
    "print(CFG.max_batches_per_epoch_train, CFG.max_batches_per_epoch_val)\n",
    "print(CFG.max_batches_per_epoch_train * CFG.batch_size, CFG.max_batches_per_epoch_val * CFG.batch_size)\n",
    "\n",
    "train_prompt_dict = {img_name: prompt for img_name, prompt in train_prompt}\n",
    "val_prompt_dict = {img_name: prompt for img_name, prompt in val_prompt}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beeedfc",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "156229ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "st_model = SentenceTransformer('../input/sentence-transformers-222/all-MiniLM-L6-v2/')\n",
    "img_model, img_preprocess = get_img_model(img_model_name=CFG.img_model_name)\n",
    "\n",
    "if CFG.train_aug:\n",
    "    train_img_preprocess = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        img_preprocess,\n",
    "    ])\n",
    "else:\n",
    "    train_img_preprocess = img_preprocess\n",
    "\n",
    "train_dataset = CustomDataSet(\n",
    "    data_dir=train_data_dir, \n",
    "    img2prompt=train_prompt_dict, \n",
    "    img_preprocess=train_img_preprocess,\n",
    ")\n",
    "val_dataset = CustomDataSet(\n",
    "    data_dir=train_data_dir, \n",
    "    img2prompt=val_prompt_dict, \n",
    "    img_preprocess=img_preprocess,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=CFG.batch_size, shuffle=True,\n",
    "                                    num_workers=CFG.num_workers)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=CFG.batch_size, shuffle=True,\n",
    "                                  num_workers=CFG.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fb0322",
   "metadata": {},
   "source": [
    "## Train  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "281fda6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_writer():\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    writer = SummaryWriter(log_dir=f\"../logs/{CFG.dataset_name}/{CFG.model_name}_{current_time}\")\n",
    "    return writer\n",
    "\n",
    "def train_epoch(img_model, st_model, train_dataloader, loss_f, optimizer, disable_tqdm=True):\n",
    "    if CFG.train_only_head:\n",
    "        img_model.eval()\n",
    "        img_model.fc.train()\n",
    "    else:\n",
    "        img_model.train()\n",
    "        \n",
    "    mean_train_loss = 0\n",
    "    train_batches_n = 0\n",
    "    for batch_i, (img_names, img_embs, prompts) in enumerate(pbar := tqdm(train_dataloader, disable=disable_tqdm)):\n",
    "        if batch_i > CFG.max_batches_per_epoch_train:\n",
    "            break\n",
    "        \n",
    "        pred = img_model(img_embs.to(CFG.device))\n",
    "        prompts_emb = torch.Tensor(st_model.encode(prompts)).to(CFG.device)\n",
    "        \n",
    "        loss = loss_f(pred, prompts_emb)\n",
    "\n",
    "        img_model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        mean_train_loss += float(loss)\n",
    "        train_batches_n += 1\n",
    "\n",
    "    mean_train_loss /= train_batches_n\n",
    "    return mean_train_loss\n",
    "\n",
    "def val_epoch(img_model, st_model, val_dataloader, loss_f, optimizer, disable_tqdm=True):\n",
    "    img_model.eval()\n",
    "    \n",
    "    mean_val_loss = 0\n",
    "    val_batches_n = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_i, (img_names, img_embs, prompts) in enumerate(pbar := tqdm(val_dataloader, disable=disable_tqdm)):\n",
    "            if batch_i > CFG.max_batches_per_epoch_val:\n",
    "                break\n",
    "                \n",
    "            img_embs = img_embs.to(CFG.device)\n",
    "                \n",
    "            pred = img_model(img_embs)\n",
    "            \n",
    "            if CFG.test_flip:\n",
    "                img_embs_flip = transforms.functional.hflip(img_embs)\n",
    "                pred_flip = img_model(img_embs_flip)\n",
    "                pred = (pred + pred_flip) / 2\n",
    "            \n",
    "            prompts_emb = torch.Tensor(st_model.encode(prompts)).to(CFG.device)\n",
    "\n",
    "            loss = loss_f(pred, prompts_emb)\n",
    "\n",
    "            mean_val_loss += float(loss)\n",
    "            val_batches_n += 1\n",
    "            \n",
    "    mean_val_loss /= val_batches_n\n",
    "    return mean_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7252358",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, lr=2.98e-06: train = 0.123, val = 0.216; new best model\n",
      "Epoch 2, lr=4.96e-06: train = 0.386, val = 0.442; new best model\n",
      "Epoch 3, lr=6.94e-06: train = 0.473, val = 0.487; new best model\n",
      "Epoch 4, lr=8.92e-06: train = 0.5, val = 0.515; new best model\n",
      "Epoch 5, lr=1.09e-05: train = 0.523, val = 0.532; new best model\n",
      "Epoch 6, lr=1.29e-05: train = 0.54, val = 0.548; new best model\n",
      "Epoch 7, lr=1.49e-05: train = 0.552, val = 0.56; new best model\n",
      "Epoch 8, lr=1.68e-05: train = 0.564, val = 0.571; new best model\n",
      "Epoch 9, lr=1.88e-05: train = 0.569, val = 0.576; new best model\n",
      "Epoch 10, lr=2.08e-05: train = 0.578, val = 0.581; new best model\n",
      "Epoch 11, lr=2.28e-05: train = 0.585, val = 0.587; new best model\n",
      "Epoch 12, lr=2.48e-05: train = 0.583, val = 0.592; new best model\n",
      "Epoch 13, lr=2.67e-05: train = 0.591, val = 0.597; new best model\n",
      "Epoch 14, lr=2.87e-05: train = 0.595, val = 0.602; new best model\n",
      "Epoch 15, lr=3.07e-05: train = 0.6, val = 0.601; continue\n",
      "Epoch 16, lr=3.27e-05: train = 0.602, val = 0.603; new best model\n",
      "Epoch 17, lr=3.47e-05: train = 0.603, val = 0.605; new best model\n",
      "Epoch 18, lr=3.66e-05: train = 0.605, val = 0.607; new best model\n",
      "Epoch 19, lr=3.86e-05: train = 0.609, val = 0.609; new best model\n",
      "Epoch 20, lr=4.06e-05: train = 0.61, val = 0.609; new best model\n",
      "Epoch 21, lr=4.26e-05: train = 0.609, val = 0.608; continue\n",
      "Epoch 22, lr=4.46e-05: train = 0.613, val = 0.609; new best model\n",
      "Epoch 23, lr=4.65e-05: train = 0.613, val = 0.613; new best model\n",
      "Epoch 24, lr=4.85e-05: train = 0.614, val = 0.611; continue\n",
      "Epoch 25, lr=5.05e-05: train = 0.615, val = 0.614; new best model\n",
      "Epoch 26, lr=5.25e-05: train = 0.612, val = 0.614; new best model\n",
      "Epoch 27, lr=5.45e-05: train = 0.614, val = 0.612; continue\n",
      "Epoch 28, lr=5.64e-05: train = 0.615, val = 0.614; continue\n",
      "Epoch 29, lr=5.84e-05: train = 0.609, val = 0.615; new best model\n",
      "Epoch 30, lr=6.04e-05: train = 0.612, val = 0.617; new best model\n",
      "Epoch 31, lr=6.24e-05: train = 0.614, val = 0.613; continue\n",
      "Epoch 32, lr=6.44e-05: train = 0.615, val = 0.616; continue\n",
      "Epoch 33, lr=6.63e-05: train = 0.617, val = 0.615; continue\n",
      "Epoch 34, lr=6.83e-05: train = 0.616, val = 0.609; continue\n",
      "Epoch 35, lr=7.03e-05: train = 0.615, val = 0.616; continue\n",
      "Epoch 36, lr=7.23e-05: train = 0.614, val = 0.616; continue\n",
      "Epoch 37, lr=7.43e-05: train = 0.616, val = 0.613; continue\n",
      "Epoch 38, lr=7.62e-05: train = 0.619, val = 0.611; continue\n",
      "Epoch 39, lr=7.82e-05: train = 0.616, val = 0.611; continue\n",
      "Epoch 40, lr=8.02e-05: train = 0.617, val = 0.613; continue\n",
      "Epoch 41, lr=8.22e-05: train = 0.615, val = 0.612; continue\n",
      "Epoch 42, lr=8.42e-05: train = 0.615, val = 0.616; continue\n",
      "Epoch 43, lr=8.61e-05: train = 0.616, val = 0.613; continue\n",
      "Epoch 44, lr=8.81e-05: train = 0.614, val = 0.612; continue\n",
      "Epoch 45, lr=9.01e-05: train = 0.612, val = 0.612; continue\n",
      "Epoch 46, lr=9.21e-05: train = 0.616, val = 0.612; continue\n",
      "Epoch 47, lr=9.41e-05: train = 0.618, val = 0.614; continue\n",
      "Epoch 48, lr=9.60e-05: train = 0.616, val = 0.609; continue\n",
      "Epoch 49, lr=9.80e-05: train = 0.613, val = 0.615; continue\n",
      "Epoch 50, lr=1.00e-04: train = 0.615, val = 0.613; continue\n",
      "Epoch 51, lr=9.80e-05: train = 0.614, val = 0.612; continue\n",
      "Epoch 52, lr=9.60e-05: train = 0.615, val = 0.613; continue\n",
      "Epoch 53, lr=9.41e-05: train = 0.615, val = 0.616; continue\n",
      "Epoch 54, lr=9.21e-05: train = 0.618, val = 0.617; continue\n",
      "Epoch 55, lr=9.01e-05: train = 0.621, val = 0.615; continue\n",
      "Epoch 56, lr=8.81e-05: train = 0.619, val = 0.618; new best model\n",
      "Epoch 57, lr=8.61e-05: train = 0.622, val = 0.615; continue\n",
      "Epoch 58, lr=8.42e-05: train = 0.622, val = 0.62; new best model\n",
      "Epoch 59, lr=8.22e-05: train = 0.623, val = 0.621; new best model\n",
      "Epoch 60, lr=8.02e-05: train = 0.625, val = 0.621; new best model\n",
      "Epoch 61, lr=7.82e-05: train = 0.625, val = 0.621; continue\n",
      "Epoch 62, lr=7.62e-05: train = 0.626, val = 0.623; new best model\n",
      "Epoch 63, lr=7.43e-05: train = 0.629, val = 0.626; new best model\n",
      "Epoch 64, lr=7.23e-05: train = 0.63, val = 0.628; new best model\n",
      "Epoch 65, lr=7.03e-05: train = 0.631, val = 0.626; continue\n",
      "Epoch 66, lr=6.83e-05: train = 0.629, val = 0.628; new best model\n",
      "Epoch 67, lr=6.63e-05: train = 0.631, val = 0.628; new best model\n",
      "Epoch 68, lr=6.44e-05: train = 0.634, val = 0.629; new best model\n",
      "Epoch 69, lr=6.24e-05: train = 0.635, val = 0.631; new best model\n",
      "Epoch 70, lr=6.04e-05: train = 0.635, val = 0.631; continue\n",
      "Epoch 71, lr=5.84e-05: train = 0.636, val = 0.633; new best model\n",
      "Epoch 72, lr=5.64e-05: train = 0.64, val = 0.634; new best model\n",
      "Epoch 73, lr=5.45e-05: train = 0.641, val = 0.636; new best model\n",
      "Epoch 74, lr=5.25e-05: train = 0.641, val = 0.634; continue\n",
      "Epoch 75, lr=5.05e-05: train = 0.64, val = 0.638; new best model\n",
      "Epoch 76, lr=4.85e-05: train = 0.641, val = 0.637; continue\n",
      "Epoch 77, lr=4.65e-05: train = 0.645, val = 0.637; continue\n",
      "Epoch 78, lr=4.46e-05: train = 0.644, val = 0.64; new best model\n",
      "Epoch 79, lr=4.26e-05: train = 0.642, val = 0.639; continue\n",
      "Epoch 80, lr=4.06e-05: train = 0.647, val = 0.64; new best model\n",
      "Epoch 81, lr=3.86e-05: train = 0.649, val = 0.642; new best model\n",
      "Epoch 82, lr=3.66e-05: train = 0.647, val = 0.643; new best model\n",
      "Epoch 83, lr=3.47e-05: train = 0.649, val = 0.64; continue\n",
      "Epoch 84, lr=3.27e-05: train = 0.648, val = 0.644; new best model\n",
      "Epoch 85, lr=3.07e-05: train = 0.653, val = 0.643; continue\n",
      "Epoch 86, lr=2.87e-05: train = 0.65, val = 0.645; new best model\n",
      "Epoch 87, lr=2.67e-05: train = 0.653, val = 0.645; continue\n",
      "Epoch 88, lr=2.48e-05: train = 0.654, val = 0.644; continue\n",
      "Epoch 89, lr=2.28e-05: train = 0.656, val = 0.646; new best model\n",
      "Epoch 90, lr=2.08e-05: train = 0.656, val = 0.647; new best model\n",
      "Epoch 91, lr=1.88e-05: train = 0.661, val = 0.645; continue\n",
      "Epoch 92, lr=1.68e-05: train = 0.659, val = 0.648; new best model\n",
      "Epoch 93, lr=1.49e-05: train = 0.655, val = 0.648; continue\n",
      "Epoch 94, lr=1.29e-05: train = 0.659, val = 0.649; new best model\n",
      "Epoch 95, lr=1.09e-05: train = 0.659, val = 0.653; new best model\n",
      "Epoch 96, lr=8.92e-06: train = 0.661, val = 0.652; continue\n",
      "Epoch 97, lr=6.94e-06: train = 0.66, val = 0.65; continue\n",
      "Epoch 98, lr=4.96e-06: train = 0.661, val = 0.654; new best model\n",
      "Epoch 99, lr=2.98e-06: train = 0.662, val = 0.652; continue\n",
      "Epoch 100, lr=1.00e-06: train = 0.66, val = 0.653; continue\n",
      "Epoch 101, lr=1.99e-06: train = 0.662, val = 0.65; continue\n",
      "Epoch 102, lr=2.98e-06: train = 0.664, val = 0.652; continue\n",
      "Epoch 103, lr=3.97e-06: train = 0.662, val = 0.652; continue\n",
      "Epoch 104, lr=4.96e-06: train = 0.664, val = 0.651; continue\n",
      "Epoch 105, lr=5.95e-06: train = 0.664, val = 0.652; continue\n",
      "Epoch 106, lr=6.94e-06: train = 0.666, val = 0.652; continue\n",
      "Epoch 107, lr=7.93e-06: train = 0.662, val = 0.652; continue\n",
      "Epoch 108, lr=8.92e-06: train = 0.663, val = 0.654; new best model\n",
      "Epoch 109, lr=9.91e-06: train = 0.664, val = 0.653; continue\n",
      "Epoch 110, lr=1.09e-05: train = 0.662, val = 0.649; continue\n",
      "Epoch 111, lr=1.19e-05: train = 0.661, val = 0.654; continue\n",
      "Epoch 112, lr=1.29e-05: train = 0.663, val = 0.653; continue\n",
      "Epoch 113, lr=1.39e-05: train = 0.665, val = 0.653; continue\n",
      "Epoch 114, lr=1.49e-05: train = 0.664, val = 0.65; continue\n",
      "Epoch 115, lr=1.58e-05: train = 0.664, val = 0.652; continue\n",
      "Epoch 116, lr=1.68e-05: train = 0.664, val = 0.654; continue\n",
      "Epoch 117, lr=1.78e-05: train = 0.665, val = 0.653; continue\n",
      "Epoch 118, lr=1.88e-05: train = 0.664, val = 0.654; continue\n",
      "Epoch 119, lr=1.98e-05: train = 0.665, val = 0.651; continue\n",
      "Epoch 120, lr=2.08e-05: train = 0.663, val = 0.652; continue\n",
      "Epoch 121, lr=2.18e-05: train = 0.662, val = 0.653; continue\n",
      "Epoch 122, lr=2.28e-05: train = 0.665, val = 0.651; continue\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f4912fdbd00>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rv/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1479, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/rv/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1443, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/usr/lib/python3.10/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 931, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/lib/python3.10/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\", line 3457, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_236048/1361756759.py\", line 17, in <module>\n",
      "    mean_train_loss = train_epoch(img_model, st_model, train_dataloader, loss_f, optimizer, disable_tqdm=True)\n",
      "  File \"/tmp/ipykernel_236048/861239834.py\", line 28, in train_epoch\n",
      "    mean_train_loss += float(loss)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\", line 2077, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/usr/lib/python3/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.10/inspect.py\", line 1667, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.10/inspect.py\", line 1625, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
      "    module = getmodule(object, filename)\n",
      "  File \"/usr/lib/python3.10/inspect.py\", line 879, in getmodule\n",
      "    if file in modulesbyfile:\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_236048/1361756759.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_epoch_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mmean_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mst_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable_tqdm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mmean_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mst_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable_tqdm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_236048/861239834.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(img_model, st_model, train_dataloader, loss_f, optimizer, disable_tqdm)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mmean_train_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mtrain_batches_n\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2076\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2077\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2078\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2077\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2078\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2079\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2080\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "writer = create_summary_writer()\n",
    "\n",
    "loss_f = get_loss(loss_name=CFG.loss_name)\n",
    "optimizer = torch.optim.Adam(img_model.parameters(), lr=CFG.lr)\n",
    "\n",
    "lr_scheduler = get_scheduler(CFG.lr_scheduler_name)\n",
    "if lr_scheduler:\n",
    "    lr_scheduler = lr_scheduler(optimizer)\n",
    "\n",
    "img_model.to(CFG.device)\n",
    "st_model.to(CFG.device)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "best_epoch_i = 0\n",
    "\n",
    "for epoch_i in range(CFG.max_epoch_num):\n",
    "    mean_train_loss = train_epoch(img_model, st_model, train_dataloader, loss_f, optimizer, disable_tqdm=True)\n",
    "    mean_val_loss = val_epoch(img_model, st_model, val_dataloader, loss_f, optimizer, disable_tqdm=True)\n",
    "    \n",
    "    if lr_scheduler:\n",
    "        lr_scheduler.step()\n",
    "    \n",
    "    train_sim = round(1 - mean_train_loss, 3)\n",
    "    val_sim = round(1 - mean_val_loss, 3)\n",
    "    \n",
    "    ### SAVE BEST MODEL ###\n",
    "    print(f\"Epoch {epoch_i + 1}, lr={optimizer.param_groups[0]['lr']:.2e}: train = {train_sim}, val = {val_sim}\", end=\"; \")\n",
    "    \n",
    "    if mean_val_loss < best_val_loss:\n",
    "        best_epoch_i = epoch_i\n",
    "        best_val_loss = mean_val_loss\n",
    "        \n",
    "        if CFG.save_model:\n",
    "            torch.save(\n",
    "                img_model.state_dict(), f\"../input/{CFG.train_files_dir}/{CFG.train_name}.torch\"\n",
    "            )\n",
    "        print(f'new best model')\n",
    "    elif epoch_i - best_epoch_i > CFG.early_stopping_patience:\n",
    "        print(f'early stopping')\n",
    "        break\n",
    "    else:\n",
    "        print(\"continue\")\n",
    "        \n",
    "    ### HISTORY ###\n",
    "    writer.add_scalars(\n",
    "        \"Similarity\",\n",
    "        {\"train\": 1 - mean_train_loss, \"val\": 1 - mean_val_loss}, \n",
    "        global_step=epoch_i + 1\n",
    "    )\n",
    "    writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954c40e7",
   "metadata": {},
   "source": [
    "## Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc5a0d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'img_256_1280_ratio_1_prompt_5_100_dupl_20_model_regnet_y_16gf_cosine_lr_1e_05_sch_None'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_model, img_preprocess = get_img_model(img_model_name=CFG.img_model_name)\n",
    "img_model.load_state_dict(torch.load(f\"../input/{CFG.train_files_dir}/{CFG.train_name}.torch\"))\n",
    "CFG.train_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd9d49a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dir = Path(\"../input/stable-diffusion-image-to-prompts/images/\")\n",
    "test_image_names = sorted(os.listdir(test_data_dir))\n",
    "test_prompt_dict = {img_name: \"\" for img_name in test_image_names}\n",
    "\n",
    "test_dataset = CustomDataSet(   \n",
    "    data_dir=test_data_dir, \n",
    "    img2prompt=test_prompt_dict, \n",
    "    img_preprocess=img_preprocess\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=CFG.batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=CFG.num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c33c2e01",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deabfb3a467e4b2cbd81bc0f2332d0cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_model.eval()\n",
    "pred_arr = []\n",
    "with torch.no_grad():\n",
    "    for img_names, img_embs, prompts in tqdm(test_dataloader):\n",
    "        prompts_emb = img_model(img_embs.to(CFG.device))\n",
    "        pred_arr.extend(prompts_emb.cpu().detach().numpy())\n",
    "pred_arr = np.array(pred_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86655bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imgId_eId</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20057f34d_0</th>\n",
       "      <td>0.087353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20057f34d_1</th>\n",
       "      <td>0.822619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20057f34d_2</th>\n",
       "      <td>1.290800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20057f34d_3</th>\n",
       "      <td>-0.150833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20057f34d_4</th>\n",
       "      <td>1.020432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f27825b2c_379</th>\n",
       "      <td>0.155445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f27825b2c_380</th>\n",
       "      <td>0.874851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f27825b2c_381</th>\n",
       "      <td>-0.456304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f27825b2c_382</th>\n",
       "      <td>-0.542236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f27825b2c_383</th>\n",
       "      <td>0.875452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2688 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    val\n",
       "imgId_eId              \n",
       "20057f34d_0    0.087353\n",
       "20057f34d_1    0.822619\n",
       "20057f34d_2    1.290800\n",
       "20057f34d_3   -0.150833\n",
       "20057f34d_4    1.020432\n",
       "...                 ...\n",
       "f27825b2c_379  0.155445\n",
       "f27825b2c_380  0.874851\n",
       "f27825b2c_381 -0.456304\n",
       "f27825b2c_382 -0.542236\n",
       "f27825b2c_383  0.875452\n",
       "\n",
       "[2688 rows x 1 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = create_submission(pred_arr, test_image_names)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b92383",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
