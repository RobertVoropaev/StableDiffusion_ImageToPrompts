{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a6ca39d",
   "metadata": {},
   "source": [
    "# 3.ImgDBSearch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6833fca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import json\n",
    "import glob\n",
    "import faiss\n",
    "import gc\n",
    "import random\n",
    "import time\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt \n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from scipy.spatial import distance\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.io import read_image\n",
    "from torchvision.models import vit_b_16, ViT_B_16_Weights, regnet_y_32gf, RegNet_Y_32GF_Weights\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "sys.path.append('../input/sentence-transformers-222/sentence-transformers')\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485247ae",
   "metadata": {},
   "source": [
    "# Config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "417acb5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'parts_100_model_regnet_y_32gf_index_faiss_flat_ip_sims_400'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size_config = {\n",
    "    \"regnet_y_32gf\": {\n",
    "        True: 16,\n",
    "        False: 8\n",
    "    },\n",
    "    \"vit_b_16\": {\n",
    "        True: 16,\n",
    "        False: 8\n",
    "    },\n",
    "    \n",
    "}\n",
    "    \n",
    "class CFG:\n",
    "    seed = 42\n",
    "    text_emb_size = 384\n",
    "    is_kaggle = (os.environ.get('PWD') == '/kaggle/working')\n",
    "    \n",
    "    img_dataset_parts = 100\n",
    "    img_model_test_size = 0.01\n",
    "    \n",
    "    img_model_name = \"regnet_y_32gf\" # \"vit_b_16\", \"regnet_y_32gf\"\n",
    "    img_model_del_head = False\n",
    "    img_emb_size = 1000\n",
    "    \n",
    "    index_name = \"faiss_flat_ip\" # \"faiss_flat_l2\", \"faiss_flat_ip\"\n",
    "    normalize_emb = True\n",
    "    \n",
    "    sim_img_k = 400\n",
    "    weight_sim_mode = \"standart_scaler\" # \"standart_scaler\", minmax_scaler\", \"mean\"\n",
    "    \n",
    "    batch_size = batch_size_config[img_model_name][is_kaggle]\n",
    "    num_workers = batch_size if not is_kaggle else 4\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    train_files_dir = \"imgdbsearch-data\"\n",
    "    train_name = f\"parts_{img_dataset_parts}_model_{img_model_name}_index_{index_name}\"\n",
    "    version_name = f\"{train_name}_sims_{sim_img_k}\"\n",
    "\n",
    "CFG.version_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ea65a7",
   "metadata": {},
   "source": [
    "# Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01ab1464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sim(emb1, emb2):\n",
    "    sim_res = 0\n",
    "    for i in range(len(emb1)):\n",
    "        sim_res += 1 - distance.cosine(emb1[i], emb2[i])\n",
    "    return sim_res / (i + 1)\n",
    "\n",
    "def get_img_model(img_model_name = CFG.img_model_name):\n",
    "    if img_model_name == \"regnet_y_32gf\":\n",
    "        if CFG.is_kaggle:\n",
    "            model = regnet_y_32gf()\n",
    "            model.load_state_dict(torch.load(f\"../input/{CFG.train_files_dir}/regnet_y_32gf_swag-04fdfa75.pth\"))\n",
    "        else:\n",
    "            weights = RegNet_Y_32GF_Weights.IMAGENET1K_SWAG_E2E_V1\n",
    "            model = regnet_y_32gf(weights=weights)\n",
    "\n",
    "        if CFG.img_model_del_head:\n",
    "            model.fc = torch.nn.Identity()\n",
    "\n",
    "        model.to(CFG.device)\n",
    "        model.eval()\n",
    "\n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.Resize(384, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "            transforms.CenterCrop(384),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    elif img_model_name == \"vit_b_16\":\n",
    "        if CFG.is_kaggle:\n",
    "            model = vit_b_16(image_size=384)\n",
    "            model.load_state_dict(torch.load(f\"../input/{CFG.train_files_dir}/vit_b_16_swag-9ac1b537.pth\"))\n",
    "        else:\n",
    "            weights = ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1\n",
    "            model = vit_b_16(weights=weights)\n",
    "\n",
    "        if CFG.img_model_del_head:\n",
    "            model.fc = torch.nn.Identity()\n",
    "\n",
    "        model.to(CFG.device)\n",
    "        model.eval()\n",
    "\n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.Resize(384, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "            transforms.CenterCrop(384),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    return model, preprocess\n",
    "\n",
    "def create_index(index_name = CFG.index_name):\n",
    "    if index_name == \"faiss_flat_l2\":\n",
    "        index = faiss.IndexFlatL2(CFG.img_emb_size)\n",
    "    elif index_name == \"faiss_flat_ip\":\n",
    "        index = faiss.IndexFlatIP(CFG.img_emb_size)\n",
    "    return index\n",
    "\n",
    "def get_sim_weight(sim_dist_arr, weight_sim_mode = CFG.weight_sim_mode):\n",
    "    k = CFG.sim_img_k\n",
    "    batch_size = sim_dist_arr.shape[0]\n",
    "    \n",
    "    if weight_sim_mode == \"standart_scaler\":\n",
    "        m = sim_dist_arr.mean(axis=1).repeat(k).reshape(-1, k)\n",
    "        s = sim_dist_arr.std(axis=1).repeat(k).reshape(-1, k)\n",
    "        sim_norm = (sim_dist_arr - m) / s\n",
    "        e = np.exp(-sim_norm).sum(axis=1).repeat(k).reshape(-1, k)\n",
    "        w = np.exp(-sim_norm) / e\n",
    "        \n",
    "    elif weight_sim_mode == \"minmax_scaler\":\n",
    "        max_ = sim_dist_arr.max(axis=1).repeat(k).reshape(-1, k)\n",
    "        min_ = sim_dist_arr.min(axis=1).repeat(k).reshape(-1, k)\n",
    "        sim_norm = (sim_dist_arr - min_) / (max_ - min_)\n",
    "        e = np.exp(-sim_norm).sum(axis=1).repeat(k).reshape(-1, k)\n",
    "        w = np.exp(-sim_norm) / e\n",
    "        \n",
    "    elif weight_sim_mode == \"mean\":\n",
    "        w = np.ones((batch_size, k)) / k\n",
    "\n",
    "    return w\n",
    "\n",
    "class CustomDataSet(Dataset):\n",
    "    def __init__(self, data_dir, img_names, transform):\n",
    "        self.data_dir = data_dir\n",
    "        self.img_names = img_names\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_names[idx]\n",
    "        img_path = os.path.join(self.data_dir, img_name)\n",
    "        image = Image.open(img_path)\n",
    "        tensor_image = self.transform(image)\n",
    "        return img_name, tensor_image\n",
    "    \n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "def create_submission(pred_arr, img_names):\n",
    "    imgIds = [i.split('.')[0] for i in img_names]\n",
    "\n",
    "    EMBEDDING_LENGTH = CFG.text_emb_size\n",
    "    eIds = list(range(EMBEDDING_LENGTH))\n",
    "\n",
    "    imgId_eId = [\n",
    "        '_'.join(map(str, i)) for i in zip(\n",
    "            np.repeat(imgIds, EMBEDDING_LENGTH),\n",
    "            np.tile(range(EMBEDDING_LENGTH), len(imgIds)))]\n",
    "    \n",
    "    submission = pd.DataFrame(\n",
    "                    index=imgId_eId,\n",
    "                    data=np.array(pred_arr).flatten(),\n",
    "                    columns=['val']).rename_axis('imgId_eId')\n",
    "    return submission\n",
    "\n",
    "def is_english_only(string):\n",
    "    for s in string:\n",
    "        cat = unicodedata.category(s)         \n",
    "        if (cat not in ['Ll', 'Lu', 'Nd', 'Po', 'Pd', 'Zs']) or (not cat.isascii()):\n",
    "            return False\n",
    "        \n",
    "    return True\n",
    "\n",
    "def filter_metadata(df, \n",
    "                    img_size_min, img_size_max, \n",
    "                    img_max_ratio_diff, \n",
    "                    prompt_words_min, prompt_words_max):\n",
    "    df = df.copy()\n",
    "    \n",
    "    df[\"size_ratio\"] = df[\"height\"] / df[\"width\"]\n",
    "    df['prompt'] = df['prompt'].str.strip()\n",
    "    df[\"num_words\"] = df['prompt'].str.split(\" \").apply(len)\n",
    "    df[\"is_english\"] = df[\"prompt\"].apply(is_english_only)\n",
    "    \n",
    "    img_hw_cond = (\n",
    "        df[\"width\"].between(img_size_min, img_size_max) & \n",
    "        df[\"height\"].between(img_size_min, img_size_max)\n",
    "    )\n",
    "    img_ratio_cond = df[\"size_ratio\"].between(1/img_max_ratio_diff, img_max_ratio_diff)\n",
    "    prompt_empty_cond = (df[\"prompt\"] != \"\")\n",
    "    prompt_num_words_cond = df[\"num_words\"].between(prompt_words_min, prompt_words_max)\n",
    "    prompt_eng_cond = df[\"is_english\"]\n",
    "\n",
    "    return df[\n",
    "        img_hw_cond &\n",
    "        img_ratio_cond &\n",
    "        prompt_empty_cond &\n",
    "        prompt_num_words_cond &\n",
    "        prompt_eng_cond\n",
    "    ]\n",
    "\n",
    "set_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62ead2b",
   "metadata": {},
   "source": [
    "# Train test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "875190b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = Path(\"../input/DiffusionDB_2M/\")\n",
    "\n",
    "metadata = pd.read_parquet(train_data_dir / \"metadata.parquet\")\n",
    "metadata = metadata[metadata[\"part_id\"] <= CFG.img_dataset_parts]\n",
    "\n",
    "full_prompt = metadata[[\"image_name\", \"prompt\"]].values\n",
    "train_prompt, val_prompt = train_test_split(\n",
    "    full_prompt, \n",
    "    test_size=CFG.img_model_test_size, \n",
    "    random_state=CFG.seed,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "train_prompt_dict = {img_name: prompt for img_name, prompt in train_prompt}\n",
    "val_prompt_dict = {img_name: prompt for img_name, prompt in val_prompt}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff67393e",
   "metadata": {},
   "source": [
    "# Get img emb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf83c73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = get_img_model(img_model_name=CFG.img_model_name)\n",
    "train_dataset = CustomDataSet(train_data_dir, list(train_prompt_dict.keys()), preprocess)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=CFG.batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=CFG.num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "582f1ea4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66096ae8ba264d76ba576e64bea8e767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_prompts = []\n",
    "index = create_index(index_name=CFG.index_name)\n",
    "for img_names, img_arr in tqdm(train_dataloader):  \n",
    "    img_arr = img_arr.to(CFG.device)\n",
    "\n",
    "    img_emb_arr = model(img_arr).cpu().detach().numpy()\n",
    "    \n",
    "    if CFG.normalize_emb:\n",
    "        img_emb_arr = img_emb_arr / np.linalg.norm(img_emb_arr)\n",
    "    index.add(img_emb_arr)\n",
    "    \n",
    "    for i in range(img_emb_arr.shape[0]):\n",
    "        train_prompts.append(train_prompt_dict[img_names[i]])\n",
    "\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6107a7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../input/{CFG.train_files_dir}/train_prompts_{CFG.train_name}.pickle\", \"wb\") as f:\n",
    "    pickle.dump(train_prompts, f)\n",
    "    \n",
    "faiss.write_index(index, f\"../input/{CFG.train_files_dir}/train_index_{CFG.train_name}.faiss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76115558",
   "metadata": {},
   "source": [
    "# Validate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79d8f1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../input/{CFG.train_files_dir}/train_prompts_{CFG.train_name}.pickle\", \"rb\") as f:\n",
    "    train_prompts = pickle.load(f)\n",
    "\n",
    "index = faiss.read_index(f\"../input/{CFG.train_files_dir}/train_index_{CFG.train_name}.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea1aa310",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = CustomDataSet(train_data_dir, list(val_prompt_dict.keys()), preprocess)\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=CFG.batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=CFG.num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fd198cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c0df7ddcd544d5baa4506d08010643c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1901523/3935506139.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mstart_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0msim_prompts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_prompts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msim_i\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msim_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msim_index_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0msim_prompt_emb_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mst_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim_prompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mtime_get_sim_prompt_emb\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1901523/3935506139.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mstart_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0msim_prompts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_prompts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msim_i\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msim_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msim_index_arr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0msim_prompt_emb_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mst_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim_prompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mtime_get_sim_prompt_emb\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_i\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "st_model = SentenceTransformer('../input/sentence-transformers-222/all-MiniLM-L6-v2/')\n",
    "\n",
    "sim_sum = 0\n",
    "img_count = 0\n",
    "batch_count = 0\n",
    "\n",
    "time_full = 0\n",
    "time_get_img_emb = 0\n",
    "time_get_sim = 0\n",
    "time_get_true_prompts = 0\n",
    "time_get_pred_prompts = 0\n",
    "time_get_sim_prompt_emb = 0\n",
    "time_get_sim_emb_mean = 0\n",
    "time_calc_sim = 0\n",
    "for img_names, img_arr in tqdm(val_dataloader):  \n",
    "    start_full = time.time()\n",
    "    \n",
    "    start = time.time()\n",
    "    img_arr = img_arr.to(CFG.device)\n",
    "    img_emb_arr = model(img_arr).cpu().detach().numpy()\n",
    "    time_get_img_emb += time.time() - start\n",
    "    \n",
    "    start = time.time()\n",
    "    if CFG.normalize_emb:\n",
    "        img_emb_arr = img_emb_arr / np.linalg.norm(img_emb_arr)\n",
    "    index.add(img_emb_arr)\n",
    "    sim_dist_arr, sim_index_arr = index.search(img_emb_arr, k=CFG.sim_img_k)\n",
    "    sim_weight_arr = get_sim_weight(sim_dist_arr, weight_sim_mode=CFG.weight_sim_mode)\n",
    "    time_get_sim += time.time() - start\n",
    "    \n",
    "    start = time.time()\n",
    "    true_prompts = [val_prompt_dict[img_name] for img_name in img_names]       \n",
    "    true_prompt_emb = st_model.encode(true_prompts)\n",
    "    time_get_true_prompts += time.time() - start\n",
    "    \n",
    "    start = time.time()\n",
    "    for i in range(len(img_names)):\n",
    "        start_i = time.time()\n",
    "        sim_prompts = [train_prompts[sim_i] for sim_i in sim_index_arr[i]]\n",
    "        sim_prompt_emb_arr = st_model.encode(sim_prompts, show_progress_bar=False)\n",
    "        time_get_sim_prompt_emb += time.time() - start_i\n",
    "        \n",
    "        start_i = time.time()\n",
    "        sim_prompt_emb_arr *= sim_weight_arr[i].repeat(CFG.text_emb_size).reshape(-1, CFG.text_emb_size)\n",
    "        sim_prompt_emb_mean = sim_prompt_emb_arr.sum(axis=0)\n",
    "        time_get_sim_emb_mean += time.time() - start_i\n",
    "    \n",
    "        start_i = time.time()\n",
    "        sim_sum += get_sim(true_prompt_emb[i], sim_prompt_emb_mean)\n",
    "        time_calc_sim += time.time() - start_i\n",
    "        \n",
    "        img_count += 1\n",
    "    time_get_pred_prompts += time.time() - start\n",
    "    \n",
    "    time_full += time.time() - start_full\n",
    "    \n",
    "    batch_count += 1\n",
    "    gc.collect()\n",
    "    \n",
    "\n",
    "print(\"Full time: \", time_full * 1000 // batch_count , \" ms\")\n",
    "print(\"Get img emb: \", time_get_img_emb * 1000 // batch_count , \" ms\")\n",
    "print(\"Get sim emb: \", time_get_sim * 1000 // batch_count, \" ms\")\n",
    "print(\"Get true emb: \", time_get_true_prompts * 1000 // batch_count, \" ms\")\n",
    "print(\"Get pred emb, full: \", time_get_pred_prompts * 1000 // batch_count, \" ms\")\n",
    "print(\"\\t batch size: \", CFG.batch_size)\n",
    "print(\"\\t sim emb: \", time_get_sim_prompt_emb * 1000 // img_count, \" ms\")\n",
    "print(\"\\t emb mean: \", time_get_sim_emb_mean * 1000 // img_count, \" ms\")\n",
    "print(\"\\t calc sim: \", time_calc_sim * 1000 // img_count, \" ms\")\n",
    "print(\"Val score: \", sim_sum / img_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baab4c2b",
   "metadata": {},
   "source": [
    "# Infer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31bbc2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, preprocess = get_img_model()\n",
    "\n",
    "index = faiss.read_index(f\"../input/{CFG.train_files_dir}/train_index_{CFG.train_name}_l2.faiss\")\n",
    "\n",
    "st_model = SentenceTransformer('../input/sentence-transformers-222/all-MiniLM-L6-v2/').to(CFG.device)\n",
    "\n",
    "with open(f\"../input/{CFG.train_files_dir}/train_prompts_{CFG.train_name}.pickle\", \"rb\") as f:\n",
    "    train_prompts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4ba4724",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dir = Path(\"../input/stable-diffusion-image-to-prompts/images/\")\n",
    "test_image_names = sorted(os.listdir(test_data_dir))\n",
    "\n",
    "test_dataset = CustomDataSet(test_data_dir, test_image_names, preprocess)\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=CFG.batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=CFG.num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee5ffd7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14c6717d69fc412b8f6196e9025d5245",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_arr = []\n",
    "\n",
    "for img_names, img_arr in tqdm(test_dataloader):  \n",
    "    img_arr = img_arr.to(CFG.device)\n",
    "\n",
    "    img_emb_arr = model(img_arr).cpu().detach().numpy()\n",
    "    \n",
    "    sim_dist_arr, sim_index_arr = index.search(img_emb_arr, k=CFG.sim_img_k)\n",
    "    sim_weight_arr = get_sim_weight(sim_dist_arr, weight_sim_mode=CFG.weight_sim_mode)\n",
    "    \n",
    "    for i in range(len(img_names)):\n",
    "        sim_prompts = [train_prompts[sim_i] for sim_i in sim_index_arr[i]]\n",
    "        sim_prompt_emb_arr = st_model.encode(sim_prompts, show_progress_bar=False)\n",
    "        \n",
    "        sim_prompt_emb_arr *= sim_weight_arr[i].repeat(CFG.text_emb_size).reshape(-1, CFG.text_emb_size)\n",
    "        sim_prompt_emb_mean = sim_prompt_emb_arr.sum(axis=0)\n",
    "        \n",
    "        pred_arr.append(sim_prompt_emb_mean)\n",
    "    \n",
    "    gc.collect()\n",
    "        \n",
    "pred_arr = np.array(pred_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "46dfec00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>val</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>imgId_eId</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20057f34d_0</th>\n",
       "      <td>-0.039952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20057f34d_1</th>\n",
       "      <td>0.008582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20057f34d_2</th>\n",
       "      <td>0.011542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20057f34d_3</th>\n",
       "      <td>0.007509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20057f34d_4</th>\n",
       "      <td>-0.019421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f27825b2c_379</th>\n",
       "      <td>0.015785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f27825b2c_380</th>\n",
       "      <td>0.061255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f27825b2c_381</th>\n",
       "      <td>0.006290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f27825b2c_382</th>\n",
       "      <td>-0.025300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f27825b2c_383</th>\n",
       "      <td>0.019092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2688 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    val\n",
       "imgId_eId              \n",
       "20057f34d_0   -0.039952\n",
       "20057f34d_1    0.008582\n",
       "20057f34d_2    0.011542\n",
       "20057f34d_3    0.007509\n",
       "20057f34d_4   -0.019421\n",
       "...                 ...\n",
       "f27825b2c_379  0.015785\n",
       "f27825b2c_380  0.061255\n",
       "f27825b2c_381  0.006290\n",
       "f27825b2c_382 -0.025300\n",
       "f27825b2c_383  0.019092\n",
       "\n",
       "[2688 rows x 1 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = create_submission(pred_arr, test_image_names)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4619b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.is_kaggle:\n",
    "    submission.to_csv(\"submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
