{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3d86772",
   "metadata": {},
   "source": [
    "# 8.I2E Ensamble "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c5f6821",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-12 21:18:54.354671: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-12 21:18:54.380884: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-12 21:18:54.722642: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import json\n",
    "import glob\n",
    "import gc\n",
    "import random\n",
    "import time\n",
    "import unicodedata\n",
    "import traceback\n",
    "import datetime\n",
    "import copy\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt \n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from scipy.spatial import distance\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models import (\n",
    "    vit_b_16, ViT_B_16_Weights, \n",
    "    vit_l_16, ViT_L_16_Weights,\n",
    "    vit_h_14, ViT_H_14_Weights,\n",
    "    regnet_y_32gf, RegNet_Y_32GF_Weights,\n",
    "    regnet_y_128gf, RegNet_Y_128GF_Weights,\n",
    "    regnet_y_16gf, RegNet_Y_16GF_Weights,\n",
    "    efficientnet_v2_l, EfficientNet_V2_L_Weights,\n",
    "    efficientnet_v2_m, EfficientNet_V2_M_Weights,\n",
    "    convnext_large, ConvNeXt_Large_Weights,\n",
    "    swin_v2_b, Swin_V2_B_Weights\n",
    ")\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import (\n",
    "    StepLR, MultiStepLR, \n",
    "    ConstantLR, LinearLR, \n",
    "    ExponentialLR, PolynomialLR, \n",
    "    CosineAnnealingLR, CosineAnnealingWarmRestarts, \n",
    "    CyclicLR, OneCycleLR, \n",
    "    ReduceLROnPlateau\n",
    ")\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0550093e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_config = {\n",
    "    \"vit_b_16\": {\n",
    "        True: 256,\n",
    "        False: 16\n",
    "    },\n",
    "    \"vit_b_16_linear\": {\n",
    "        True: 256,\n",
    "        False: 48\n",
    "    },\n",
    "    \"regnet_y_16gf\": {\n",
    "        True: 64,\n",
    "        False: 26\n",
    "    },\n",
    "    \"regnet_y_16gf_linear\": {\n",
    "        True: 64,\n",
    "        False: 26\n",
    "    },\n",
    "    \"regnet_y_32gf\": {\n",
    "        True: 16,\n",
    "        False: 6\n",
    "    },\n",
    "    \"regnet_y_32gf_linear\": {\n",
    "        True: 16,\n",
    "        False: 20\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efeab113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_model(img_model_name: str, load_weight: bool, head_emb_size: int):\n",
    "    if img_model_name == \"regnet_y_16gf\":\n",
    "        if not load_weight:\n",
    "            model = regnet_y_16gf()\n",
    "        else:\n",
    "            weights = RegNet_Y_16GF_Weights.IMAGENET1K_SWAG_E2E_V1\n",
    "            model = regnet_y_16gf(weights=weights)\n",
    "        model.fc = torch.nn.Linear(3024, head_emb_size)\n",
    "        \n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.Resize(224, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    if img_model_name == \"regnet_y_16gf_linear\":\n",
    "        if not load_weight:\n",
    "            model = regnet_y_16gf()\n",
    "        else:\n",
    "            weights = RegNet_Y_16GF_Weights.IMAGENET1K_SWAG_LINEAR_V1\n",
    "            model = regnet_y_16gf(weights=weights)\n",
    "        model.fc = torch.nn.Linear(3024, head_emb_size)\n",
    "        \n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.Resize(224, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    elif img_model_name == \"regnet_y_32gf\":\n",
    "        if not load_weight:\n",
    "            model = regnet_y_32gf()\n",
    "        else:\n",
    "            weights = RegNet_Y_32GF_Weights.IMAGENET1K_SWAG_E2E_V1\n",
    "            model = regnet_y_32gf(weights=weights)\n",
    "        model.fc = torch.nn.Linear(3712, head_emb_size)\n",
    "        \n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.Resize(384, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "            transforms.CenterCrop(384),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        \n",
    "    elif img_model_name == \"regnet_y_32gf_linear\":\n",
    "        if not load_weight:\n",
    "            model = regnet_y_32gf()\n",
    "        else:\n",
    "            weights = RegNet_Y_32GF_Weights.IMAGENET1K_SWAG_LINEAR_V1\n",
    "            model = regnet_y_32gf(weights=weights)\n",
    "        model.fc = torch.nn.Linear(3712, head_emb_size)\n",
    "        \n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.Resize(224, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        \n",
    "    elif img_model_name == \"vit_b_16\":\n",
    "        if not load_weight:\n",
    "            model = vit_b_16(image_size=384)\n",
    "        else:\n",
    "            weights = ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1\n",
    "            model = vit_b_16(weights=weights)\n",
    "        model.heads.head = torch.nn.Linear(768, head_emb_size)\n",
    "        \n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.Resize(384, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "            transforms.CenterCrop(384),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    \n",
    "    elif img_model_name == \"vit_b_16_linear\":\n",
    "        if not load_weight:\n",
    "            model = vit_b_16(image_size=224)\n",
    "        else:\n",
    "            weights = ViT_B_16_Weights.IMAGENET1K_SWAG_LINEAR_V1\n",
    "            model = vit_b_16(weights=weights)\n",
    "        model.heads.head = torch.nn.Linear(768, head_emb_size)\n",
    "        \n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.Resize(224, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    \n",
    "    return model, preprocess\n",
    "\n",
    "def create_submission(pred_arr, img_names, text_emb_size):\n",
    "    imgIds = [i.split('.')[0] for i in img_names]\n",
    "\n",
    "    EMBEDDING_LENGTH = text_emb_size\n",
    "    eIds = list(range(EMBEDDING_LENGTH))\n",
    "\n",
    "    imgId_eId = [\n",
    "        '_'.join(map(str, i)) for i in zip(\n",
    "            np.repeat(imgIds, EMBEDDING_LENGTH),\n",
    "            np.tile(range(EMBEDDING_LENGTH), len(imgIds)))]\n",
    "    \n",
    "    submission = pd.DataFrame(\n",
    "                    index=imgId_eId,\n",
    "                    data=np.array(pred_arr).flatten(),\n",
    "                    columns=['val']).rename_axis('imgId_eId')\n",
    "    return submission\n",
    "\n",
    "class CustomDataSet(Dataset):\n",
    "    def __init__(self, data_dir, img2prompt, img_preprocess):\n",
    "        self.data_dir = data_dir\n",
    "        self.img_names = list(img2prompt.keys())\n",
    "        self.img2prompt = img2prompt\n",
    "        self.img_preprocess = img_preprocess\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_names[idx]\n",
    "        img_path = os.path.join(self.data_dir, img_name)\n",
    "        img = Image.open(img_path)\n",
    "        img_emb = self.img_preprocess(img)\n",
    "        \n",
    "        prompt = str(self.img2prompt[img_name])\n",
    "        \n",
    "        return img_name, img_emb, prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b94754bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class CFG_CLASS:\n",
    "    dataset_dupl_word: int\n",
    "    img_model_name: str \n",
    "    lr_scheduler_name: str\n",
    "    lr: float\n",
    "    \n",
    "    seed: int = 42\n",
    "    text_emb_size: int = 384\n",
    "    is_kaggle: bool = (os.environ.get('PWD') == '/kaggle/working')\n",
    "    train_files_dir: str = \"img2emb-data\"\n",
    "    \n",
    "    save_model: bool = True\n",
    "    img_model_test_size: float = 0.05\n",
    "\n",
    "    loss_name: str = \"cosine\"\n",
    "    train_only_head: bool = False\n",
    "    \n",
    "    train_aug: bool = True\n",
    "    test_flip: bool = True\n",
    "    \n",
    "    full_train_epoch_num: int = 100\n",
    "    max_epoch_num: int = full_train_epoch_num * 10\n",
    "    full_val_epoch_num: int = 2\n",
    "    early_stopping_patience: int = full_val_epoch_num * 25\n",
    "    \n",
    "    device: torch.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    dataset_name: str = field(init=False)\n",
    "    metadata_path: str = field(init=False)\n",
    "    aug_name: str = field(init=False)\n",
    "    model_name: str = field(init=False)\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.dataset_name = f\"dataset_duplwords_{self.dataset_dupl_word}\"\n",
    "        self.metadata_path = f\"../input/metadata/metadata_duplwords_{self.dataset_dupl_word}.parquet\"\n",
    "        \n",
    "        self.aug_name: str = f\"flip_{int(self.train_aug)}\"\n",
    "        self.model_name = f\"model_{self.img_model_name}_sch_{self.lr_scheduler_name}_lr_{self.lr:.0e}\".replace(\"-\", \"_\")\n",
    "        \n",
    "        self.train_name = f\"{self.dataset_name}_{self.model_name}\"\n",
    "        \n",
    "        self.batch_size = batch_size_config[self.img_model_name][self.is_kaggle]\n",
    "        self.num_workers = self.batch_size if not self.is_kaggle else 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91e403ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_config(train_size, val_size, batch_size, full_train_epoch_num, full_val_epoch_num):\n",
    "    max_batches_per_epoch_train = train_size // batch_size // full_train_epoch_num\n",
    "    max_batches_per_epoch_val = val_size // batch_size // full_val_epoch_num\n",
    "    return max_batches_per_epoch_train, max_batches_per_epoch_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4ca3070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = CFG_CLASS(\n",
    "    dataset_dupl_word=5,\n",
    "    img_model_name=\"regnet_y_32gf_linear\",\n",
    "    lr=5,\n",
    "    lr_scheduler_name=\"None\",\n",
    "    save_model=True,\n",
    ")\n",
    "set_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "de067680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata shape:  (503293, 2)\n",
      "Train/val\n",
      "Sizes:  478128 / 25165\n",
      "Batches per epoch:  239 / 629\n",
      "Images per epoch:  4780 / 12580\n"
     ]
    }
   ],
   "source": [
    "train_data_dir = Path(\"../input/\")\n",
    "metadata = pd.read_parquet(CFG.metadata_path)\n",
    "print(\"Metadata shape: \", metadata.shape)\n",
    "\n",
    "full_prompt = metadata[[\"image_name\", \"prompt\"]].values\n",
    "train_prompt, val_prompt = train_test_split(\n",
    "    full_prompt, \n",
    "    test_size=CFG.img_model_test_size, \n",
    "    random_state=CFG.seed,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "CFG.dataset_train_size = len(train_prompt)\n",
    "CFG.dataset_val_size = len(val_prompt)\n",
    "CFG.max_batches_per_epoch_train, CFG.max_batches_per_epoch_val = get_train_config(\n",
    "    train_size=CFG.dataset_train_size, \n",
    "    val_size=CFG.dataset_val_size, \n",
    "    batch_size=CFG.batch_size, \n",
    "    full_train_epoch_num=CFG.full_train_epoch_num, \n",
    "    full_val_epoch_num=CFG.full_val_epoch_num\n",
    ")\n",
    "\n",
    "print(\"Train/val\")\n",
    "print(\"Sizes: \", CFG.dataset_train_size, \"/\", CFG.dataset_val_size)\n",
    "print(\"Batches per epoch: \", \n",
    "      CFG.max_batches_per_epoch_train, \"/\",\n",
    "      CFG.max_batches_per_epoch_val)\n",
    "print(\"Images per epoch: \", \n",
    "      CFG.max_batches_per_epoch_train * CFG.batch_size, \"/\",\n",
    "      CFG.max_batches_per_epoch_val * CFG.batch_size)\n",
    "\n",
    "train_prompt_dict = {img_name: prompt for img_name, prompt in train_prompt}\n",
    "val_prompt = val_prompt[:10000]\n",
    "val_prompt_dict = {img_name: prompt for img_name, prompt in val_prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1b9093cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    seed = 42\n",
    "    text_emb_size = 384\n",
    "    is_kaggle = (os.environ.get('PWD') == '/kaggle/working')\n",
    "    \n",
    "    train_files_dir = \"img2emb-data\"\n",
    "    \n",
    "    test_flip = True\n",
    "    \n",
    "    # RESOURCES\n",
    "    num_workers = 2\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model_names = [\n",
    "        \"vit_b_16\", \"vit_b_16_linear\", \"regnet_y_16gf\", \"regnet_y_32gf\", \"regnet_y_16gf_linear\", \"regnet_y_32gf_linear\", \n",
    "    ]\n",
    "    model_scores = [\n",
    "        0.52504, 0.52275, 0.53276, 0.52972, 0.53928, 0.54452\n",
    "    ]\n",
    "    \n",
    "    model_alphas = [\n",
    "        1, 1, 1, 1, 1, 1\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c1ba8cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../input/sentence-transformers-222/sentence-transformers')\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "st_model = SentenceTransformer('../input/sentence-transformers-222/all-MiniLM-L6-v2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b23cfb40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3385567dfe16429894828c49ae72fa45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dcbda7a570e482e8f46affc4092c1a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/209 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9da93f8840241849ddee477a7d1f44c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/385 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "108f16c6717642c8ae4a9ff4e94eb531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1667 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c5cb1c8fdd472c818860f5dd5d702d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/385 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78afdfdbacf8402691942c1dba2b9dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data_dir = train_data_dir\n",
    "test_image_names = os.listdir(test_data_dir)\n",
    "test_prompt_dict = val_prompt_dict\n",
    "\n",
    "pred_arr_models_list = []\n",
    "\n",
    "for model_name in CFG.model_names:\n",
    "    img_model, img_preprocess = get_img_model(img_model_name=model_name, \n",
    "                                              load_weight=False, \n",
    "                                              head_emb_size=CFG.text_emb_size)\n",
    "    \n",
    "    model_path = f\"../input/{CFG.train_files_dir}/dataset_duplwords_5_model_{model_name}_sch_None_lr_1e_05.torch\"\n",
    "    img_model.load_state_dict(torch.load(model_path))\n",
    "    img_model.to(CFG.device)\n",
    "    img_model.eval()\n",
    "    \n",
    "    test_dataset = CustomDataSet(   \n",
    "        data_dir=test_data_dir, \n",
    "        img2prompt=test_prompt_dict, \n",
    "        img_preprocess=img_preprocess\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size_config[model_name][False], \n",
    "        shuffle=False, \n",
    "        num_workers=CFG.num_workers\n",
    "    )\n",
    "    \n",
    "    pred_arr_model = [] \n",
    "    true_emb = []\n",
    "    with torch.no_grad():\n",
    "        for img_names, img_embs, prompts in tqdm(test_dataloader):\n",
    "            img_embs = img_embs.to(CFG.device)\n",
    "            pred = img_model(img_embs) # (batch, emb_size)\n",
    "            \n",
    "            if CFG.test_flip:\n",
    "                img_embs_flip = transforms.functional.hflip(img_embs)\n",
    "                pred_flip = img_model(img_embs_flip)\n",
    "                pred = (pred + pred_flip) / 2\n",
    "            \n",
    "            pred = pred.cpu().detach().numpy()\n",
    "                \n",
    "            pred_arr_model.extend(pred) \n",
    "            true_emb.extend(st_model.encode(prompts))\n",
    "                \n",
    "    pred_arr_model = np.array(pred_arr_model) # (images, emb_size)\n",
    "    pred_arr_models_list.append(pred_arr_model) # (models, images, emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "97b06fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3840000, 6)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_arr_flatten_list = [pred.flatten() for pred in pred_arr_models_list]\n",
    "pred_arr_flatten = np.vstack(pred_arr_flatten_list).T\n",
    "pred_arr_flatten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2a973b8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3840000,)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_flatten = np.array(true_emb).flatten()\n",
    "true_flatten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5aec37bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SGDRegressor(fit_intercept=False)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SGDRegressor</label><div class=\"sk-toggleable__content\"><pre>SGDRegressor(fit_intercept=False)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SGDRegressor(fit_intercept=False)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "lr = SGDRegressor(fit_intercept=False, )\n",
    "lr.fit(pred_arr_flatten, true_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1bee9995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regnet_y_16gf_linear\t\t0.53928\t\t0.3146333928496869\n",
      "vit_b_16\t\t0.52504\t\t0.4281575193038673\n",
      "vit_b_16_linear\t\t0.52275\t\t0.5060009186607468\n",
      "regnet_y_32gf_linear\t\t0.54452\t\t0.6600851190851034\n",
      "regnet_y_16gf\t\t0.53276\t\t0.7230120487436666\n",
      "regnet_y_32gf\t\t0.52972\t\t0.9170895882734724\n"
     ]
    }
   ],
   "source": [
    "for n, s, c in sorted(zip(CFG.model_names, CFG.model_scores, lr.coef_), key=lambda x: x[2]):\n",
    "    print(n, s, c*100, sep=\"\\t\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bb077984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vit_b_16\t\t0.52504\t\t0.4281575193038673\n",
      "vit_b_16_linear\t\t0.52275\t\t0.5060009186607468\n",
      "regnet_y_16gf\t\t0.53276\t\t0.7230120487436666\n",
      "regnet_y_32gf\t\t0.52972\t\t0.9170895882734724\n",
      "regnet_y_16gf_linear\t\t0.53928\t\t0.3146333928496869\n",
      "regnet_y_32gf_linear\t\t0.54452\t\t0.6600851190851034\n"
     ]
    }
   ],
   "source": [
    "for n, s, c in zip(CFG.model_names, CFG.model_scores, lr.coef_):\n",
    "    print(n, s, c*100, sep=\"\\t\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1d09d4",
   "metadata": {},
   "source": [
    "#  By score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b201bdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = CFG_CLASS(\n",
    "    dataset_dupl_word=5,\n",
    "    img_model_name=\"regnet_y_32gf_linear\",\n",
    "    lr=5,\n",
    "    lr_scheduler_name=\"None\",\n",
    "    save_model=True,\n",
    ")\n",
    "set_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "ce8462ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata shape:  (503293, 2)\n",
      "Train/val\n",
      "Sizes:  478128 / 25165\n",
      "Batches per epoch:  239 / 629\n",
      "Images per epoch:  4780 / 12580\n"
     ]
    }
   ],
   "source": [
    "train_data_dir = Path(\"../input/\")\n",
    "metadata = pd.read_parquet(CFG.metadata_path)\n",
    "print(\"Metadata shape: \", metadata.shape)\n",
    "\n",
    "full_prompt = metadata[[\"image_name\", \"prompt\"]].values\n",
    "train_prompt, val_prompt = train_test_split(\n",
    "    full_prompt, \n",
    "    test_size=CFG.img_model_test_size, \n",
    "    random_state=CFG.seed,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "CFG.dataset_train_size = len(train_prompt)\n",
    "CFG.dataset_val_size = len(val_prompt)\n",
    "CFG.max_batches_per_epoch_train, CFG.max_batches_per_epoch_val = get_train_config(\n",
    "    train_size=CFG.dataset_train_size, \n",
    "    val_size=CFG.dataset_val_size, \n",
    "    batch_size=CFG.batch_size, \n",
    "    full_train_epoch_num=CFG.full_train_epoch_num, \n",
    "    full_val_epoch_num=CFG.full_val_epoch_num\n",
    ")\n",
    "\n",
    "print(\"Train/val\")\n",
    "print(\"Sizes: \", CFG.dataset_train_size, \"/\", CFG.dataset_val_size)\n",
    "print(\"Batches per epoch: \", \n",
    "      CFG.max_batches_per_epoch_train, \"/\",\n",
    "      CFG.max_batches_per_epoch_val)\n",
    "print(\"Images per epoch: \", \n",
    "      CFG.max_batches_per_epoch_train * CFG.batch_size, \"/\",\n",
    "      CFG.max_batches_per_epoch_val * CFG.batch_size)\n",
    "\n",
    "train_prompt_dict = {img_name: prompt for img_name, prompt in train_prompt}\n",
    "val_prompt = val_prompt[:10000]\n",
    "val_prompt_dict = {img_name: prompt for img_name, prompt in val_prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a652a0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    seed = 42\n",
    "    text_emb_size = 384\n",
    "    is_kaggle = (os.environ.get('PWD') == '/kaggle/working')\n",
    "    \n",
    "    train_files_dir = \"img2emb-data\"\n",
    "    \n",
    "    test_flip = True\n",
    "    \n",
    "    # RESOURCES\n",
    "    num_workers = 2\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model_names = [\n",
    "        \"vit_b_16\", \"vit_b_16_linear\", \"regnet_y_16gf\", \"regnet_y_32gf\", \"regnet_y_16gf_linear\", \"regnet_y_32gf_linear\", \n",
    "    ]\n",
    "    model_scores = [\n",
    "        0.52504, 0.52275, 0.53276, 0.52972, 0.53928, 0.54452\n",
    "    ]\n",
    "    \n",
    "    model_alphas = [\n",
    "        1, 1, 1, 1, 1, 1\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "804a7c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "141f8f4bfe6d4507a1e67d67a615548e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e76d8766a7a44202816fd5a11e3787aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/209 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b0768e25fd4c20bdf9ebd2f9b85b6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/385 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da2671b93f0e465e91b2ced5184b59b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1667 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1675c68c5ea4f5eba810af427016b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/385 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e31e207f0fb54f37acef71e597d8e17b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data_dir = train_data_dir\n",
    "test_image_names = os.listdir(test_data_dir)\n",
    "test_prompt_dict = val_prompt_dict\n",
    "\n",
    "pred_arr_models_list = []\n",
    "\n",
    "for model_name in CFG.model_names:\n",
    "    img_model, img_preprocess = get_img_model(img_model_name=model_name, \n",
    "                                              load_weight=False, \n",
    "                                              head_emb_size=CFG.text_emb_size)\n",
    "    \n",
    "    model_path = f\"../input/{CFG.train_files_dir}/dataset_duplwords_5_model_{model_name}_sch_None_lr_1e_05.torch\"\n",
    "    img_model.load_state_dict(torch.load(model_path))\n",
    "    img_model.to(CFG.device)\n",
    "    img_model.eval()\n",
    "    \n",
    "    test_dataset = CustomDataSet(   \n",
    "        data_dir=test_data_dir, \n",
    "        img2prompt=test_prompt_dict, \n",
    "        img_preprocess=img_preprocess\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=batch_size_config[model_name][False], \n",
    "        shuffle=False, \n",
    "        num_workers=CFG.num_workers\n",
    "    )\n",
    "    \n",
    "    pred_arr_model = [] \n",
    "    true_emb = []\n",
    "    with torch.no_grad():\n",
    "        for img_names, img_embs, prompts in tqdm(test_dataloader):\n",
    "            img_embs = img_embs.to(CFG.device)\n",
    "            pred = img_model(img_embs) # (batch, emb_size)\n",
    "            \n",
    "            if CFG.test_flip:\n",
    "                img_embs_flip = transforms.functional.hflip(img_embs)\n",
    "                pred_flip = img_model(img_embs_flip)\n",
    "                pred = (pred + pred_flip) / 2\n",
    "            \n",
    "            pred = pred.cpu().detach().numpy()\n",
    "                \n",
    "            pred_arr_model.extend(pred) \n",
    "            true_emb.extend(st_model.encode(prompts))\n",
    "                \n",
    "    pred_arr_model = np.array(pred_arr_model) # (images, emb_size)\n",
    "    pred_arr_models_list.append(pred_arr_model) # (models, images, emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5689bf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(loss_name, device):\n",
    "    if loss_name == \"cosine\":\n",
    "        loss_fn = torch.nn.CosineEmbeddingLoss()\n",
    "        return lambda pred, true: loss_fn(pred.to(device), true.to(device), torch.ones(pred.size(0)).to(device))\n",
    "    \n",
    "loss = get_loss(\"cosine\", CFG.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "77493e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_emb = np.array(true_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "74a56559",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "0be6fdad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 384)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_arr_models_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1128d046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7785928215114327a39d481155c1a9f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6360373714691414 (0.0, 0.0, 0.0, 0.0, 0.0, 0.2)\n",
      "0.636037371469589 (0.0, 0.0, 0.0, 0.0, 0.0, 0.4)\n",
      "0.636037371469701 (0.0, 0.0, 0.0, 0.0, 0.0, 0.8)\n",
      "0.6460583992867055 (0.0, 0.0, 0.0, 0.0, 0.2, 0.2)\n",
      "0.6460583992868194 (0.0, 0.0, 0.0, 0.0, 0.4, 0.4)\n",
      "0.6462382327271485 (0.0, 0.0, 0.0, 0.0, 0.4, 0.6000000000000001)\n",
      "0.6462983750600009 (0.0, 0.0, 0.0, 0.0, 0.6000000000000001, 0.8)\n",
      "0.6473434412852217 (0.0, 0.0, 0.0, 0.2, 0.0, 0.2)\n",
      "0.6519247276995987 (0.0, 0.0, 0.0, 0.2, 0.2, 0.2)\n",
      "0.6525792136065508 (0.0, 0.0, 0.0, 0.4, 0.2, 0.2)\n",
      "0.6525792136066012 (0.0, 0.0, 0.0, 0.8, 0.4, 0.4)\n",
      "0.6526310065901999 (0.0, 0.0, 0.0, 1.0, 0.6000000000000001, 0.6000000000000001)\n",
      "0.6534682281288349 (0.0, 0.0, 0.2, 0.2, 0.0, 0.2)\n",
      "0.6539065814320673 (0.0, 0.0, 0.2, 0.2, 0.2, 0.2)\n",
      "0.6547815235482886 (0.0, 0.0, 0.2, 0.4, 0.2, 0.2)\n",
      "0.6549707218938094 (0.0, 0.0, 0.4, 0.6000000000000001, 0.2, 0.4)\n",
      "0.6549953720944401 (0.0, 0.0, 0.6000000000000001, 1.0, 0.4, 0.6000000000000001)\n",
      "0.6551278842473358 (0.0, 0.2, 0.0, 0.4, 0.2, 0.2)\n",
      "0.6553478772609793 (0.0, 0.2, 0.2, 0.2, 0.0, 0.2)\n",
      "0.6554232171792446 (0.0, 0.2, 0.2, 0.2, 0.2, 0.2)\n",
      "0.6561038106159912 (0.0, 0.2, 0.2, 0.4, 0.0, 0.2)\n",
      "0.6564841277009728 (0.0, 0.2, 0.2, 0.4, 0.2, 0.2)\n",
      "0.6565014598006225 (0.0, 0.2, 0.2, 0.6000000000000001, 0.2, 0.2)\n",
      "0.6566841847218132 (0.0, 0.2, 0.4, 0.6000000000000001, 0.2, 0.4)\n",
      "0.6567090162433787 (0.0, 0.4, 0.4, 0.8, 0.2, 0.4)\n",
      "0.6567141153676316 (0.0, 0.4, 0.6000000000000001, 1.0, 0.2, 0.4)\n",
      "0.656791601491602 (0.0, 0.4, 0.6000000000000001, 1.0, 0.2, 0.6000000000000001)\n",
      "0.6568164959708851 (0.2, 0.0, 0.2, 0.4, 0.2, 0.2)\n",
      "0.6569103976910444 (0.2, 0.0, 0.2, 0.6000000000000001, 0.2, 0.2)\n",
      "0.6569391028180052 (0.2, 0.0, 0.2, 0.6000000000000001, 0.2, 0.4)\n",
      "0.65713018962263 (0.2, 0.0, 0.4, 0.6000000000000001, 0.2, 0.4)\n",
      "0.6571324237276486 (0.2, 0.0, 0.4, 0.8, 0.2, 0.4)\n",
      "0.6572831643246004 (0.2, 0.2, 0.2, 0.6000000000000001, 0.2, 0.2)\n",
      "0.6574525548209602 (0.2, 0.2, 0.2, 0.6000000000000001, 0.2, 0.4)\n",
      "0.6574980154369371 (0.2, 0.2, 0.2, 0.8, 0.2, 0.4)\n",
      "0.6576513383922988 (0.2, 0.2, 0.4, 0.6000000000000001, 0.2, 0.4)\n",
      "0.6577898301689076 (0.2, 0.2, 0.4, 0.8, 0.2, 0.4)\n"
     ]
    }
   ],
   "source": [
    "m_best = None\n",
    "s_best = 0\n",
    "\n",
    "n_images = pred_arr_models_list[0].shape[0]\n",
    "n_models = len(pred_arr_models_list)\n",
    "\n",
    "model_alphas_list = product(np.linspace(0, 1, 6), repeat=6)\n",
    "for model_alphas in tqdm(model_alphas_list):\n",
    "    img2emb_pred_arr = np.zeros((n_images, CFG.text_emb_size))\n",
    "    for j in range(n_models):\n",
    "        img2emb_pred_arr += pred_arr_models_list[j] * model_alphas[j] / sum(CFG.model_alphas)\n",
    "    s = 1 - float(loss(torch.tensor(img2emb_pred_arr), torch.tensor(true_emb)))\n",
    "    \n",
    "    if s > s_best:\n",
    "        s_best = s\n",
    "        m_best = model_alphas\n",
    "        print(s, model_alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "697180a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.2, 0.2, 0.4, 0.8, 0.2, 0.4), 0.6577763351279051)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_best, s_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0c12aed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c06aa473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dc09a4b24e14188872192d2400a78a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/729 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6577123514786629 (0.15, 0.15, 0.35, 0.75, 0.15, 0.35)\n",
      "0.6577463180680058 (0.15, 0.2, 0.35, 0.75, 0.15, 0.35)\n",
      "0.6577715660789212 (0.2, 0.15, 0.35, 0.75, 0.15, 0.35)\n",
      "0.6577743558078629 (0.2, 0.15, 0.4, 0.75, 0.15, 0.35)\n",
      "0.6577784906571984 (0.2, 0.2, 0.35, 0.75, 0.15, 0.35)\n",
      "0.6577887378473193 (0.2, 0.2, 0.4, 0.75, 0.15, 0.35)\n",
      "0.6577897141620852 (0.2, 0.2, 0.4, 0.75, 0.15, 0.4)\n",
      "0.6577898301689076 (0.2, 0.2, 0.4, 0.8, 0.2, 0.4)\n"
     ]
    }
   ],
   "source": [
    "m_best = None\n",
    "s_best = 0\n",
    "\n",
    "n_images = pred_arr_models_list[0].shape[0]\n",
    "n_models = len(pred_arr_models_list)\n",
    "\n",
    "model_alphas_list = list(\n",
    "    itertools.product(\n",
    "        [0.15, 0.2, 0.25], [0.15, 0.2, 0.25], [0.35, 0.4, 0.45], [0.75, 0.8, 0.85], [0.15, 0.2, 0.25], [0.35, 0.4, 0.45]\n",
    "    )\n",
    ")\n",
    "for model_alphas in tqdm(model_alphas_list):\n",
    "    img2emb_pred_arr = np.zeros((n_images, CFG.text_emb_size))\n",
    "    for j in range(n_models):\n",
    "        img2emb_pred_arr += pred_arr_models_list[j] * model_alphas[j] / sum(CFG.model_alphas)\n",
    "    s = 1 - float(loss(torch.tensor(img2emb_pred_arr), torch.tensor(true_emb)))\n",
    "    \n",
    "    if s > s_best:\n",
    "        s_best = s\n",
    "        m_best = model_alphas\n",
    "        print(s, model_alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b154cefd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.33333333, 1.        , 0.        ,\n",
       "       0.33333333])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([0.2, 0.2, 0.4, 0.8, 0.2, 0.4])\n",
    "(a - a.min()) / (a.max() - a.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "4e2e5003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.19672131, 0.32786885, 0.67213115, 1.        , 0.        ,\n",
       "       0.57377049])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([0.43, 0.51, 0.72, 0.92, 0.31, 0.66])\n",
    "(a - a.min()) / (a.max() - a.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "59ea768c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.20554649, 0.11215334, 0.52039152, 0.39641109, 0.7862969 ,\n",
       "       1.        ])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([0.52, 0.52504, 0.52275, 0.53276, 0.52972, 0.53928, 0.54452])\n",
    "((a - a.min()) / (a.max() - a.min())) [1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1215d511",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
